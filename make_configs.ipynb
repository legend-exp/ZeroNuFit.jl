{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d15d46ac-da13-44ce-883f-5cb91395e252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/LEGEND/ZeroNuFit.jl`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\") # Activate the environment\n",
    "using ArgParse\n",
    "using Logging, LoggingExtras\n",
    "using JSON\n",
    "using FilePathsBase\n",
    "using DataStructures\n",
    "using PropDicts\n",
    "using Tables\n",
    "using TypedTables\n",
    "using LegendDataManagement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb13e46-3c52-40e4-8e66-4fc41cd16ca5",
   "metadata": {},
   "source": [
    "### Notebook to make the configs for the fit\n",
    "This should be converted to a `script.jl` all it does it reformat inputs from L-200, GERDA and Majorana into one common format readble by `ZeroNuFit.jl`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a10f8c0-f508-48a0-9061-b54009419b0f",
   "metadata": {},
   "source": [
    "### Events files\n",
    "First we need files containing the physics events in the three experiments.\n",
    "For L-200 these are not available to the collaboration, so we need to create the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ab3c861-9a1d-4c24-b5be-bb66ce700861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1617"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function read_events_gerda(file_path)\n",
    "    timestamp=[1455109448, 1457847659, 1472522222, 1475981084, 1480290460,\n",
    "                   1485848926, 1503578885, 1509498133, 1516142805, 1533092526,\n",
    "                   1539047354, 1566823934, 1568276649]\n",
    "    detector=[\"ANG4\", \"GD61C\", \"GD35B\", \"ANG1\", \"GD35B\", \"GD91A\", \"GD76C\",\n",
    "                  \"ANG1\", \"RG1\", \"GD61C\", \"IC74A\", \"ANG4\", \"GD32D\"]\n",
    "    energy=[1995.2452, 1958.6807, 2018.1346, 1950.9419, 2067.9735,\n",
    "                2056.4280, 2042.0641, 1962.7372, 1957.5059, 1970.1398,\n",
    "                2058.8776, 2015.8751, 2012.0643]\n",
    "\n",
    "    output=Dict(\"events\" => [])\n",
    "    for (t,d,e) in zip(timestamp,detector,energy)\n",
    "        append!(output[\"events\"],[Dict(\"timestamp\"=>t,\"energy\"=>e,\"detector\"=>d)])\n",
    "    end\n",
    "    open(file_path, \"w\") do file\n",
    "        write(file, json(output,4))\n",
    "    end\n",
    "end\n",
    "\n",
    "read_events_gerda(\"config/events_gerda.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32267068-4524-4f9b-a3a3-df18a191aab2",
   "metadata": {},
   "source": [
    "### Partitions files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec3bb725-a279-4638-bb80-9213c1baa95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_partitions_file_gerda (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function make_partitions_file_gerda(input_path,output_path)\n",
    "    json_data = JSON.parsefile(input_path,dicttype=DataStructures.OrderedDict)\n",
    "    #println(JSON.json(json_data, 4))\n",
    "    partitions=[]\n",
    "    for (part,part_data) in sort(json_data)\n",
    "        for (det,info) in json_data[part]\n",
    "            if (info isa OrderedDict)\n",
    "                part_temp = OrderedDict(\"detector\"=> det,\n",
    "                                 \"start_ts\"=>json_data[part][\"start_ts\"],\n",
    "                                 \"end_ts\"=>json_data[part][\"end_ts\"])\n",
    "                copy_fields=[\"eff_tot\",\"eff_tot_sigma\",\"fwhm\",\"fwhm_sigma\",\"exposure\",\"bias\"]\n",
    "                for field in copy_fields\n",
    "                    part_temp[field]=json_data[part][det][field]\n",
    "                end\n",
    "                   \n",
    "                part_temp[\"bias_sigma\"]=json_data[part][det][\"energy_sigma\"]\n",
    "                append!(partitions,[part_temp])\n",
    "            end\n",
    "        end\n",
    "        open(output_path, \"w\") do file\n",
    "            write(file, json(partitions,4))\n",
    "        end\n",
    "    end\n",
    "                \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a7797d5-b0b8-483e-a1c1-317edd11554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_partitions_file_gerda(\"/home/tdixon/Downloads/0vbb-analysis-parameters.json\",\"config/partitions_gerda.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cb2f8c9-558e-4399-887b-49c671899540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "printdb (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function printdb(db)\n",
    "        if db isa PropDict\n",
    "            for (key,value) in db\n",
    "                println(key)\n",
    "                printdb(value)\n",
    "            end\n",
    "        else\n",
    "            println(\"\\t\",db)\n",
    "            println()\n",
    "        end\n",
    "    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9be63213-a9ad-4dc1-952a-8935850075b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_partitions_file_l200 (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function make_partitions_file_l200(meta_path,output_path)\n",
    "\n",
    "    meta = LegendDataManagement.AnyProps(meta_path)\n",
    "    partitions = meta.datasets.ovbb_partitions_pars\n",
    "    printflag=false\n",
    "    output=[]\n",
    "    # loop over detectors\n",
    "    for (detector, detdata) in partitions\n",
    "        if detector == :default\n",
    "            continue\n",
    "        end\n",
    "\n",
    "      \n",
    "        \n",
    "        # apply defaults\n",
    "        if partitions.default isa PropDicts.MissingProperty \n",
    "            detdata_merge = copy(detdata)\n",
    "        else\n",
    "            new = partitions.default\n",
    "            detdata_merge = merge(new,copy(detdata))\n",
    "        end\n",
    "        \n",
    "        # loop over partitions for this detector\n",
    "        for (partition, pardata) in detdata_merge\n",
    "            if partition == :default\n",
    "                continue\n",
    "            end\n",
    "\n",
    "            if detdata_merge.default isa PropDicts.MissingProperty \n",
    "                pardata_merge = copy(pardata)\n",
    "            else\n",
    "                new = detdata_merge.default\n",
    "                pardata_merge = merge(new,copy(pardata))\n",
    "            end\n",
    "\n",
    "            if printflag == true\n",
    "                println(\"for partition $partition \")\n",
    "                printdb(pardata_merge)\n",
    "                println(\"\\n\")\n",
    "            end\n",
    "            part_temp = OrderedDict(\"detector\"=> detector,\n",
    "                                 \"start_ts\"=>pardata_merge[:span_in_utc_s][1],\n",
    "                                 \"end_ts\"=>pardata_merge[:span_in_utc_s][2])\n",
    "            eff = prod([v[:val] for v in values(pardata_merge[:ovbb_acceptance])])\n",
    "            eff_sigma = âˆš(sum([v[:unc] for v in values(pardata_merge[:ovbb_acceptance])].^2))    \n",
    "            part_temp[\"eff_tot\"]=eff\n",
    "            part_temp[\"eff_tot_sigma\"]=eff_sigma\n",
    "            part_temp[\"fwhm\"]=pardata_merge[:fwhm_in_keV][:val]\n",
    "            part_temp[\"fwhm_sigma\"]=pardata_merge[:fwhm_in_keV][:unc]\n",
    "            part_temp[\"exposure\"]=pardata_merge[:livetime_in_s]*\n",
    "                meta.hardware.detectors.germanium.diodes[detector].production.mass_in_g/(60*60*24*365.25*1000)\n",
    "            part_temp[\"bias\"]=pardata_merge[:energy_bias_in_keV][:val]\n",
    "            part_temp[\"bias_sigma\"]=pardata_merge[:energy_bias_in_keV][:unc]\n",
    "\n",
    "            append!(output,[part_temp])\n",
    "\n",
    "        end\n",
    "    end\n",
    "    open(output_path, \"w\") do file\n",
    "        write(file, json(output,4))\n",
    "       \n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f800353-961e-4115-be17-38f56a12870a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105967"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_partitions_file_l200(\"../legend-metadata\",\"config/partitions_l200.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "328d62e5-ad28-4f3a-8c87-3e08c8890f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_partitions_new (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_partitions_new(part_path::String)\n",
    "\"\"\"\n",
    "Get the partition info from a jason file and save  to a Table\n",
    "\"\"\"\n",
    "    part_data_json = JSON.parsefile(part_path,dicttype=DataStructures.OrderedDict)\n",
    "    k = keys(part_data_json[1])\n",
    "    arrays=Dict()\n",
    "    for key in k\n",
    "        arrays[key]=[]\n",
    "    end\n",
    "    for part in part_data_json\n",
    "        for key in k\n",
    "            append!(arrays[key],[part[key]])\n",
    "            end\n",
    "\n",
    "    end\n",
    "    #TODO: find a way to make this not hardcoded\n",
    "    tab = Table(detector=Array(arrays[\"detector\"]),\n",
    "                start_ts=Array(arrays[\"start_ts\"]),\n",
    "                end_ts=Array(arrays[\"end_ts\"]),\n",
    "                eff_tot=Array(arrays[\"eff_tot\"]),\n",
    "                eff_tot_sigma=Array(arrays[\"eff_tot_sigma\"]),\n",
    "                fwhm=Array(arrays[\"fwhm\"]),\n",
    "                fwhm_sigma=Array(arrays[\"fwhm_sigma\"]),\n",
    "                exposure=Array(arrays[\"exposure\"]),\n",
    "                bias =Array(arrays[\"bias\"]),\n",
    "                bias_sigma =Array(arrays[\"bias_sigma\"]))\n",
    "\n",
    "    return tab\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c313166c-27e4-4c09-a517-19511fb6ac90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba00c6-2d10-4c80-9787-052fd71ef77a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
