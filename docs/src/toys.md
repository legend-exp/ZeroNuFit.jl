# Generating toys

Table of contents:

```@contents
Pages = ["toys.md"]
Depth = 3
```

## Posterior predictive distribution studies

In order to check for any mis-modeling in the fits and to further understand the derived half-life limits, we performed "sensitivity" studies. 
In a Bayesian context, the sensitivity is related to the concept of posterior predictive distributions.
The prior predictive distribution is the expected distribution of data coming from a future experiment identical to that performed and repeated under the same conditions.
This marginalizes the uncertainty in the model parameters, based on their prior distributions. 
Alternatively, the posterior predictive distribution weights the data by the posterior obtained from the analysis.
If the original data were modeled appropriately, then fake data generated under the given model should distribute similarly to the original data (see _A. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin, "Bayesian Data Analysis", 3rd ed. 798, Chapman & Hall, 2013_ for further details).

Considering the observed signal counts as an observable of the data, we can thus extract "sensitivity curves" that can be interpreted as the distribution of expected future limits derived from repeating the identical experiment.
This is distinct from a frequentist sensitivity since uncertainty on nuisance parameters are marginalized over.

Sensitivity studies can be run over $N_{\rm tot}$ toy spectra generated by sampling the posterior marginalized distributions of nuisance parameters obtained in the no-signal hypothesis fit, i.e. in a fit where the signal contribution was excluded from the modeling of events. 
In particular, starting from the posterior distributions derived for all nuisance parameters via the fit over original data, $p(\theta | D)$, data were sampled from a model's posterior distribution as follows:

```math
\begin{aligned}
    p(y | D) = \int p(y | \theta) p(\theta | D) d\theta
\end{aligned}
```

where $y$ are the generated data, $D$ are the observed data, and $\theta$ are the model parameters (but the signal). 
This is achieved by drawing samples of $\theta$ from the posterior distribution $p(\theta | D)$, and subsequently generating datasets $y$ based on the likelihood model $p(y | \theta)$. 
In particular, for each partition, the expected number of background events was calculated using Poisson sampling, $n_{\rm b} \sim \text{Pois}(\mu_b)$, so that $n_{\rm b}$ events could be uniformly generated in the considered fit energy window to build a toy energy spectrum.
Each generated toy energy spectrum can be later fitted with a signal+background model.

## Running toys

A module `sensitivity.jl` is present for generating toys and running sensitivity studies. The script can be run as

```
$ julia sensitivity.jl -c config_fake_data.json -i N
```

where $N=\{1,\,...,\,N_{\rm tot}\}$ is an integer number corresponding to the toy index.
The input config file (`config_fake_data.json`) has the following entries:

```
{
    "path_to_fit": "output/fit_gerda_phIandphII_NoSignal/",
    "best_fit": false,
    "low_stat": true,
    "seed": null
}

```

where
- `"path_to_fit"` is the path to the already performed fit over real data;
- `"best_fit": true` if we want to fix the paramaters to the best fit coming from the fit under the no-signal hypothesis;
- `"low_stat": true` to run 5 MCMC chains with $10^5$ steps each; if `false`, then the statistics used for the original fit under the no-signal hypothesis is used;
- `"seed": null` if we want a random seed when generating fake data, otherwise you can fix it to an Int value.

Below, we show an example of a bash script used for running sensitivity studies as multiple jobs on NERSC:

```bash
#!/bin/bash                                                                                                                                                 
#SBATCH -q regular                                                                                                                                       
#SBATCH --constraint=cpu                                                                                                                                    
#SBATCH -t 48:00:00
#SBATCH -J sens_test                                                                                                                                         
#SBATCH --output parallel.log                                                     
#SBATCH --error parallel.err  

module load parallel
module load julia
srun="srun -N 1"
parallel="parallel --delay 1 -j 128"

# run parallel jobs
$srun  $parallel "julia sensitivity.jl -c config_fake_data.json -i {1}" ::: {1..10000} &

wait
```


## Testing alternative models using already existing toys 
Another way to run the code is present if, for instance, an user wants to use toy data generated according to one model and fit them with a different model.
In this case, the path to the folder containing the already existing JSON files with toy energy events previously generated has to be provided together with the toy index:

```
julia sensitivity.jl -c config_fake_data.json -i N --path_to_toys path_to_your_toys
```

Below, an example of a bash script that can be used for retrieving multiple existing toy data and running sensitivity studies as multiple jobs on NERSC:

```bash
#!/bin/bash                                                                                                                                                 
#SBATCH -q regular                                                                                                                                       
#SBATCH --constraint=cpu                                                                                                                                    
#SBATCH -t 48:00:00
#SBATCH -J sens_test                                                                                                                                         
#SBATCH --output parallel.log                                                     
#SBATCH --error parallel.err             

# set the directory path to toys
path_to_toys="path_to_your_toys"
all_files=("$path_to_toys"/*.json)
full_paths=()
for file in "${all_files[@]}"; do
    if [[ -f "$file" ]]; then 
        full_paths+=("$file")
    fi
done
if [ ${#full_paths[@]} -eq 0 ]; then
    echo "The list of existing toy data is empty! Exit here."
    exit 1
else
    echo "You are going to run a fit over ${#full_paths[@]} number of already existing toys stored under $path_to_toys"
fi

# array to hold toy_idx
toy_indices=()

# loop over available fake JSON toys
for path in "${full_paths[@]}"; do
    base_name="${path%.json}"
    number_str="${base_name##*fake_data}"  
    toy_idx=$((number_str))  

    toy_indices+=("$toy_idx") 
done
echo "List of toy indices: ${toy_indices[*]}"

module load parallel
module load julia
srun="srun -N 1"
parallel="parallel --delay 1 -j 128"
$srun $parallel "julia sensitivity.jl -c config/toy_9_l200_1BI_new_data_same_bkg_noS.json -i {1}" ::: "${toy_indices[@]}"

wait
```



## Testing fake scanerios
In the above examples, you can replace the julia-running line in order to test the "sensitivity" of the experiment for fixed input partitions parameters and a fixed level of background (`<bkg_index>`: expressed in units of $10^{-4}$ counts/keV/kg/yr) for the exclusion scenario:

```bash
$srun  $parallel "julia sensitivity.jl -c config/fake_config.json -i {1} -f true -b <bkg_idex>" ::: {1..10000} 
```

!!! warning

    You can find an example of `fake_config.json` under `config/`. Notice that the option `"bkg_only"` has to be set to `true` for the code to run. In this way, the code is able to generate a fake toy spectrum assuming no signal, and then it will automatically include a signal contribution in the model when it comes to fit the generated toy events.

An example of fake_partitions.json` input is the following, where you fill entries with the values you want to test the $0\nu\beta\beta$ sensitivity:

```json
{
    "fit_groups": {
        "all_l200a": {
            "range": [
                [1930.0, 2098.511], [2108.511, 2113.513], [2123.513, 2190.0}
            ],
            "model": "uniform",
            "bkg_name": "B_l200a_all"
        }
    },
    "partitions": {
        "all_l200a": [
            {
            "experiment": "L200",
            "detector": "FAKEDET", // random name
            "part_name": "part0001", // partition ID
            "start_ts": 1704950367, // used to map the enregy events to the correct partition ID
            "end_ts": 1708271505, // used to map the enregy events to the correct partition ID
            "eff_tot": 0.69,
            "eff_tot_sigma": 0, 
            "width": 1.0616522503600239, // energy resolution in standard deviation (in keV)
            "width_sigma": 0,
            "fwhm": 2.5, // energy resolution in FWHM (in keV)
            "fwhm_sigma": 0,
            "exposure": 1000,
            "bias": 0, 
            "bias_sigma": 0
            }
        ]
    }
}
```

!!! warning

    The code is working for 1 fake partition only. If in `fake_partitions.json` you add more than one fake partition, all partitions but the first one will be ignored when generatind your toy spectra.


The code works for deriving the exclusion sensitivity, so for the `fake_events.json` input just use:
```json
{
    "events": [
    ]
}
```

The implementation of a discovery sensitivity study has not been carried on.