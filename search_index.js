var documenterSearchIndex = {"docs":
[{"location":"installation/#First-steps","page":"First steps","title":"First steps","text":"","category":"section"},{"location":"installation/#How-to-run-the-code","page":"First steps","title":"How to run the code","text":"","category":"section"},{"location":"installation/","page":"First steps","title":"First steps","text":"Run the following command by specifying the path to the configuration file used for settings:","category":"page"},{"location":"installation/","page":"First steps","title":"First steps","text":"$ julia main.jl -c config/config.json","category":"page"},{"location":"installation/#Julia-Project-enviroments","page":"First steps","title":"Julia Project enviroments","text":"","category":"section"},{"location":"installation/","page":"First steps","title":"First steps","text":"To run the code in a virtual enviroment you can use the following. Start by entering the Julia command view by typing julia on your terminal. Once inside, enter the packagge manager (via ]), activate the environment in the current directory, and resolve (and install, if necessary) dependencies:","category":"page"},{"location":"installation/","page":"First steps","title":"First steps","text":"pkg> ] \npkg> activate .\npkg> instantiate","category":"page"},{"location":"installation/","page":"First steps","title":"First steps","text":"Now you can run the script inside this enviroment with:","category":"page"},{"location":"installation/","page":"First steps","title":"First steps","text":"$ julia main.jl --project=. -c config/config.json","category":"page"},{"location":".ipynb_checkpoints/inputs-checkpoint/#Partitions-and-events","page":"Partitions and events","title":"Partitions and events","text":"","category":"section"},{"location":".ipynb_checkpoints/inputs-checkpoint/","page":"Partitions and events","title":"Partitions and events","text":"The fit takes in inputs two files in JSON format (for a full customization of the fit), which paths have to be specified in the config.json file.","category":"page"},{"location":".ipynb_checkpoints/inputs-checkpoint/","page":"Partitions and events","title":"Partitions and events","text":"Table of contents:","category":"page"},{"location":".ipynb_checkpoints/inputs-checkpoint/","page":"Partitions and events","title":"Partitions and events","text":"Pages = [\"inputs.md\"]\nDepth = 3","category":"page"},{"location":".ipynb_checkpoints/inputs-checkpoint/#Partitions-file","page":"Partitions and events","title":"Partitions file","text":"","category":"section"},{"location":".ipynb_checkpoints/inputs-checkpoint/","page":"Partitions and events","title":"Partitions and events","text":"The partitions file gives information on the independent spectra to be used in the fit/likelihood, this is set by the \"partitions\" key in the config file.  This provides all the information neccessary to define the fit model.","category":"page"},{"location":".ipynb_checkpoints/inputs-checkpoint/","page":"Partitions and events","title":"Partitions and events","text":"The file consists of a file of independent spectra to include in the fit (for example channels or partitions).  A partition is defined uniquely by a range of time-stamps, a detector name and an experiment name. ","category":"page"},{"location":".ipynb_checkpoints/inputs-checkpoint/","page":"Partitions and events","title":"Partitions and events","text":"note: Note\nIn principle the 'detector' does not need to be a single detector (e.g. C000RG1) but can be a label for any groups of detectors (e.g. COAX_dets).  This allows to make fits where all detectors are fit together.","category":"page"},{"location":".ipynb_checkpoints/inputs-checkpoint/","page":"Partitions and events","title":"Partitions and events","text":"The partitions are grouped into fit_groups: these are sets of partitions which are treated with the same background/signal fit model and range. In the partitions file, the user must provide the information on the fit groups and partitions (organized by fit group). ","category":"page"},{"location":".ipynb_checkpoints/inputs-checkpoint/","page":"Partitions and events","title":"Partitions and events","text":"This JSON file has a nested structure with two subdictionaries, the first with key \"fit_groups\", describing the groupings in the fit, and the second \"partitions\" giving a list of partitions for each fit group. An example is shown below.","category":"page"},{"location":".ipynb_checkpoints/inputs-checkpoint/","page":"Partitions and events","title":"Partitions and events","text":"{\n\"fit_groups\":{\n                \"group_one\":{\n                            \"range\":[[1930,2099],[2109,2114],[2124,2190]],\n                            \"model\":\"uniform\",\n                            \"bkg_name\":\"low_bkg\"\n                            \"signal_name\":\"gaussian_plus_lowEtail\"\n                            }\n\n},\n\"partitions\": {\n                \"group_one\":[\n\n                            {  \n                                \"experiment\": \"LEGEND\",\n                                \"detector\": \"DET_0\",\n                                \"start_ts\": 1704950367,\n                                \"end_ts\": 1708271505,\n                                \"eff_tot\": 0.6,\n                                \"eff_tot_sigma\": 0.1,\n                                \"width\": 3,\n                                \"width_sigma\": 1,\n                                \"exposure\": 1,\n                                \"bias\": 0.2,\n                                \"bias_sigma\": 0.1\n                            }, ...\n                            ],\n                \"group_two\":...\n            },\n\n\n}\n            ","category":"page"},{"location":".ipynb_checkpoints/inputs-checkpoint/","page":"Partitions and events","title":"Partitions and events","text":"They key bkg_name is used to set the name of the background parameter for this group. Note that several groups can be fitted with the same background parameter, this enables quick modification of the fit.","category":"page"},{"location":".ipynb_checkpoints/inputs-checkpoint/","page":"Partitions and events","title":"Partitions and events","text":"The key \"model\":\"uniform\" is used to set the background model to uniform as default. For different background model shapes, additional information are necessary and these can be specified in the config.json (see the \"Configuration file\" documentation).","category":"page"},{"location":".ipynb_checkpoints/inputs-checkpoint/","page":"Partitions and events","title":"Partitions and events","text":"warning: Warning\nThe background shape is set to global for all partitions, differently from the signal shape (see below) that can be specified differently for each fit group.","category":"page"},{"location":".ipynb_checkpoints/inputs-checkpoint/","page":"Partitions and events","title":"Partitions and events","text":"Notice it is possible to specify the signal shape for each fit group.  The available options at the moment are \"signal_name\":\"gaussian_plus_lowEtail\" or \"signal_name\":\"gaussian\" (default). If the key is omitted, the default Gaussian signal shape will be adopted. Notice that if you want to use the \"signal_name\":\"gaussian_plus_lowEtail\" option (eg for MAJORANA DEMONSTRATOR data), you need to provide additional input signal shape parameters (frac, sigma, tau). ","category":"page"},{"location":".ipynb_checkpoints/inputs-checkpoint/","page":"Partitions and events","title":"Partitions and events","text":"note: Note\nMAJORANA DEMONSTRATOR (MJD) input data are taken from \"I. J. Arnquist et al., Final Result of the Majorana Demonstrator’s Search for Neutrinoless Double- β Decay in Ge 76, PRL 130, 062501 (2023)\".  Here, you can find the input data under the supplemental materials: supp_analysis_parameters.txt and supp_event_list.txt.","category":"page"},{"location":".ipynb_checkpoints/inputs-checkpoint/","page":"Partitions and events","title":"Partitions and events","text":"warning: Warning\nNotice that the width parameter assumes different meanings depending on the chosen signal shape:if \"signal_name\": \"gaussian_plus_lowEtail\", width is the fractional uncertainty on the FWHM of the peak at 2039 keV (it rescales the energy resolution of the peak)\nif \"signal_name\": \"gaussian\", width is the energy resolution expressed in standard deviations (NOT in FWHM)","category":"page"},{"location":".ipynb_checkpoints/inputs-checkpoint/#Events-file","page":"Partitions and events","title":"Events file","text":"","category":"section"},{"location":".ipynb_checkpoints/inputs-checkpoint/","page":"Partitions and events","title":"Partitions and events","text":"In addition, it is neccessary to provide an 'event' file describing the events observed in the data, the path to this file is specified by the 'events' key in the config. Again this is a JSON file consisting of a list of observed events of the form.","category":"page"},{"location":".ipynb_checkpoints/inputs-checkpoint/","page":"Partitions and events","title":"Partitions and events","text":"    {       \"experiment\":\"LEGEND\",\n            \"energy\": 2069.420,\n            \"timestamp\": 1755109448,\n            \"detector\": \"DET_0\"\n        },","category":"page"},{"location":".ipynb_checkpoints/inputs-checkpoint/","page":"Partitions and events","title":"Partitions and events","text":"The timestamp and detector are used to extract which partition this event corresponds to. To convert to this format from the standard GERDA and LEGEND files, there are tools available in https://github.com/tdixon97/legend-0vbb-config.","category":"page"},{"location":".ipynb_checkpoints/inputs-checkpoint/","page":"Partitions and events","title":"Partitions and events","text":"It is possible to supply a list of partition and event files in this case the list of fit groups and events are concatenated.","category":"page"},{"location":".ipynb_checkpoints/inputs-checkpoint/","page":"Partitions and events","title":"Partitions and events","text":"warning: Warning\nIf multiple files are provided fit_group must still be unique.","category":"page"},{"location":"inputs/#Partitions-and-events","page":"Partitions and events","title":"Partitions and events","text":"","category":"section"},{"location":"inputs/","page":"Partitions and events","title":"Partitions and events","text":"The fit takes in inputs two files in JSON format (for a full customization of the fit), which paths have to be specified in the config.json file.","category":"page"},{"location":"inputs/","page":"Partitions and events","title":"Partitions and events","text":"Table of contents:","category":"page"},{"location":"inputs/","page":"Partitions and events","title":"Partitions and events","text":"Pages = [\"inputs.md\"]\nDepth = 3","category":"page"},{"location":"inputs/#Partitions-file","page":"Partitions and events","title":"Partitions file","text":"","category":"section"},{"location":"inputs/","page":"Partitions and events","title":"Partitions and events","text":"The partitions file gives information on the independent spectra to be used in the fit/likelihood, this is set by the \"partitions\" key in the config file.  This provides all the information neccessary to define the fit model.","category":"page"},{"location":"inputs/","page":"Partitions and events","title":"Partitions and events","text":"The file consists of a file of independent spectra to include in the fit (for example channels or partitions).  A partition is defined uniquely by a range of time-stamps, a detector name and an experiment name. ","category":"page"},{"location":"inputs/","page":"Partitions and events","title":"Partitions and events","text":"note: Note\nIn principle the 'detector' does not need to be a single detector (e.g. C000RG1) but can be a label for any groups of detectors (e.g. COAX_dets).  This allows to make fits where all detectors are fit together.","category":"page"},{"location":"inputs/","page":"Partitions and events","title":"Partitions and events","text":"The partitions are grouped into fit_groups: these are sets of partitions which are treated with the same background/signal fit model and range. In the partitions file, the user must provide the information on the fit groups and partitions (organized by fit group). ","category":"page"},{"location":"inputs/","page":"Partitions and events","title":"Partitions and events","text":"This JSON file has a nested structure with two subdictionaries, the first with key \"fit_groups\", describing the groupings in the fit, and the second \"partitions\" giving a list of partitions for each fit group. An example is shown below.","category":"page"},{"location":"inputs/","page":"Partitions and events","title":"Partitions and events","text":"{\n\"fit_groups\":{\n                \"group_one\":{\n                            \"range\":[[1930,2099],[2109,2114],[2124,2190]],\n                            \"model\":\"uniform\",\n                            \"bkg_name\":\"low_bkg\"\n                            \"signal_name\":\"gaussian_plus_lowEtail\"\n                            }\n\n},\n\"partitions\": {\n                \"group_one\":[\n\n                            {  \n                                \"experiment\": \"LEGEND\",\n                                \"detector\": \"DET_0\",\n                                \"start_ts\": 1704950367,\n                                \"end_ts\": 1708271505,\n                                \"eff_tot\": 0.6,\n                                \"eff_tot_sigma\": 0.1,\n                                \"width\": 3,\n                                \"width_sigma\": 1,\n                                \"exposure\": 1,\n                                \"bias\": 0.2,\n                                \"bias_sigma\": 0.1\n                            }, ...\n                            ],\n                \"group_two\":...\n            },\n\n\n}\n            ","category":"page"},{"location":"inputs/","page":"Partitions and events","title":"Partitions and events","text":"They key bkg_name is used to set the name of the background parameter for this group. Note that several groups can be fitted with the same background parameter, this enables quick modification of the fit.","category":"page"},{"location":"inputs/","page":"Partitions and events","title":"Partitions and events","text":"The key \"model\":\"uniform\" is used to set the background model to uniform as default. For different background model shapes, additional information are necessary and these can be specified in the config.json (see the \"Configuration file\" documentation).","category":"page"},{"location":"inputs/","page":"Partitions and events","title":"Partitions and events","text":"warning: Warning\nThe background shape is set to global for all partitions, differently from the signal shape (see below) that can be specified differently for each fit group.","category":"page"},{"location":"inputs/","page":"Partitions and events","title":"Partitions and events","text":"Notice it is possible to specify the signal shape for each fit group.  The available options at the moment are \"signal_name\":\"gaussian_plus_lowEtail\" or \"signal_name\":\"gaussian\" (default). If the key is omitted, the default Gaussian signal shape will be adopted. Notice that if you want to use the \"signal_name\":\"gaussian_plus_lowEtail\" option (eg for MAJORANA DEMONSTRATOR data), you need to provide additional input signal shape parameters (frac, sigma, tau). ","category":"page"},{"location":"inputs/","page":"Partitions and events","title":"Partitions and events","text":"note: Note\nMAJORANA DEMONSTRATOR (MJD) input data are taken from \"I. J. Arnquist et al., Final Result of the Majorana Demonstrator’s Search for Neutrinoless Double- β Decay in Ge 76, PRL 130, 062501 (2023)\".  Here, you can find the input data under the supplemental materials: supp_analysis_parameters.txt and supp_event_list.txt.","category":"page"},{"location":"inputs/","page":"Partitions and events","title":"Partitions and events","text":"warning: Warning\nNotice that the width parameter assumes different meanings depending on the chosen signal shape:if \"signal_name\": \"gaussian_plus_lowEtail\", width is the fractional uncertainty on the FWHM of the peak at 2039 keV (it rescales the energy resolution of the peak)\nif \"signal_name\": \"gaussian\", width is the energy resolution expressed in standard deviations (NOT in FWHM)","category":"page"},{"location":"inputs/#Events-file","page":"Partitions and events","title":"Events file","text":"","category":"section"},{"location":"inputs/","page":"Partitions and events","title":"Partitions and events","text":"In addition, it is neccessary to provide an 'event' file describing the events observed in the data, the path to this file is specified by the 'events' key in the config. Again this is a JSON file consisting of a list of observed events of the form.","category":"page"},{"location":"inputs/","page":"Partitions and events","title":"Partitions and events","text":"    {       \"experiment\":\"LEGEND\",\n            \"energy\": 2069.420,\n            \"timestamp\": 1755109448,\n            \"detector\": \"DET_0\"\n        },","category":"page"},{"location":"inputs/","page":"Partitions and events","title":"Partitions and events","text":"The timestamp and detector are used to extract which partition this event corresponds to. To convert to this format from the standard GERDA and LEGEND files, there are tools available in https://github.com/tdixon97/legend-0vbb-config.","category":"page"},{"location":"inputs/","page":"Partitions and events","title":"Partitions and events","text":"It is possible to supply a list of partition and event files in this case the list of fit groups and events are concatenated.","category":"page"},{"location":"inputs/","page":"Partitions and events","title":"Partitions and events","text":"warning: Warning\nIf multiple files are provided fit_group must still be unique.","category":"page"},{"location":"config/#Configuration-file","page":"Configuration file","title":"Configuration file","text":"","category":"section"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"Table of contents:","category":"page"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"Pages = [\"config.md\"]\nDepth = 3","category":"page"},{"location":"config/#Building-the-configuration-file","page":"Configuration file","title":"Building the configuration file","text":"","category":"section"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"Before running the code, set the input config.json file with following entries:","category":"page"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"{\n    \"debug\":false,\n    \"partitions\":[\"config/partitions_gerda_new.json\",\"config/partitions_l200.json\",\"config/partitions_mjd_new.json\"],\n    \"events\":    [\"config/events_gerda.json\",\"config/events_l200.json\",\"config/events_mjd_new_part.json\"],\n    \"output_path\": \"output/fit_mjd_l200_gerda_v2/\",\n    \"overwrite\":true,\n    \"bat_fit\": {\"nchains\": 6, \"nsteps\": 1e6},\n    \"plot\": {\n            \"fit_and_data\": false,\n            \"bandfit_and_data\": false,\n            \"scheme\":\"red\",\n            \"alpha\":0.3\n        },\n    \"bkg_only\": false,\n    \"signal\": {\"upper_bound\":1000, \"prior\": \"uniform\"},\n    \"bkg\": {\n             \"units\": \"ckky\",\n             \"upper_bound\":0.1,\n             \"prior\": \"uniform\",\n             \"correlated\": {\"mode\": \"none\", \"range\": \"none\"}\n             },\n    ...\n}","category":"page"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"where","category":"page"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"\"debug\": true if you want to display debug output on terminal;\n\"partitions\": list of partitions JSON inputs; it takes one entry per experiment;\n\"events\": list of events JSON inputs; it takes one entry per experiment;\n\"output_path\": path where to store outputs (logs, plots, mcmc results);\n\"overwrite\": true if you want to overwrite a previous fit with same output_path; if set to false but no fits were previously performed (ie there are no outputs to overwrite), the code will save the output of this fit;\n\"bat_fit\": settings for the BAT fit, \"nchains\" numer of MCMC chains to run, \"nsteps\" number of steps for each MCMC chain;\n\"plot\": settings for plotting; \"fit_and_data\": true plots fit line over data (and CI bands if \"bandfit_and_data\": true); \"scheme\":\"red\" and \"alpha\":0.3 are used for customizing output appearances;\n\"bkg_only\": true if we fit assuming no signal (S=0), false otherwise;\n\"signal\": select \"upper_bound\" for the prior and the \"prior\" shape (uniform, sqrt, ...);\n\"bkg\": select \"upper_bound\" for the prior range and the \"prior\" shape (uniform, ...). There are several optional keys with details given below: if these are not provided, the fit defaults to a flat background without correlations. Notice the units of the final BI can be chosen among \"ckky\" (counts/keV/kg/yr) or \"cFty\" (counts/FWHM/t/yr).","category":"page"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"Moreover, the config requires the following block for nuisance parameters, ie energy scale (=energy bias and resolution) and efficiency:","category":"page"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"    {\n    ...\n    \"nuisance\": { \n         \"energy_bias\": {\n            \"fixed\": false,\n            \"correlated\": false\n         },\n         \"energy_res\": {\n            \"fixed\": false,\n            \"correlated\": false\n         },\n         \"efficiency\" : {\n            \"correlated\": true,\n            \"fixed\": false\n            }\n    }","category":"page"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"In particular, you can set \"correlated\": true if you want to use one variable to correlate the nuisance parameters (eg to speed up the computation times), and \"fixed\": false if you want to include a prior for nuisance parameters (otherwise these parameters they will be fixed to their partition value and not constrained).","category":"page"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"If a variable is correlated (either energy_bias or energy_res or efficiency), the code will search for a field in the fit_groups block of the partitions JSON file to use a correlated variable per each fit group.  In particular, the field can be specified as:","category":"page"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"\"energy_bias_group_name\": \"...\"\n\"energy_reso_group_name\": \"...\"\n\"efficiency_group_name\": \"...\"","category":"page"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"Parameters are then added to the model called αr_\\$name (for resolution), αe_\\$name for efficiency and αb_\\$name for bias.","category":"page"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"warning: Warning\nThe alpha parameter names default to _all, if you want one different per experiment this must be explicitly specified in the fit groups entry","category":"page"},{"location":"config/#Background-shape-and-correlation","page":"Configuration file","title":"Background shape and correlation","text":"","category":"section"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"There are several options to control the background in more detail. These can be added to the \"bkg\" part of the config: In particular:","category":"page"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"\"correlated\" adds a hierarchical (correlated) background to the model, this key should have a dictionary giving details on the prior shape and ranges. The three options for the mode are: none (default), lognormal or normal. The range entry is associated to the range used for the uniform prior on the \\$sigma_B parameter. For example:","category":"page"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"\"correlated\": {\"mode\":\"lognormal\",\"range\":[0,1]}","category":"page"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"\"shape\" changes the shape of the background from uniform. The user should provide a dictionary giving details on the shape. For example:","category":"page"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"\"shape\": {\n            \"name\":\"exponential\",\n            \"pars\":{\"slope\":[-10,10]}\n        },","category":"page"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"The \"pars\" subdictionary describes the range of the priors on the parameters of the model, currently implemented shapes are \"uniform\", \"linear\" and \"exponential\". These names correspond to functions in fitting.jl and logical conditions in get_bkg_pdf in likelihood.jl.","category":"page"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"This will add parameters ${bkg_name}_slope or similar to the model (and then call them). This names therefore must correspond to the names in the functions in fitting.jl. To add a new shape simply define a new method in fitting.jl and a new logical condition in get_bkg_pdf in likelihood.jl.","category":"page"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"note: Note\nIf these keys are not provided the model defaults to a uniform uncorrelated background.","category":"page"},{"location":"config/#Signal-shape","page":"Configuration file","title":"Signal shape","text":"","category":"section"},{"location":"config/","page":"Configuration file","title":"Configuration file","text":"The signal shape can be specified in the partition JSON file in order to have a different signal shape for different partitions. An example is given by the combined fit of GERDA/LEGEND and MAJORANA DEMONSTRATOR (MJD) partitions where for the first one we want to model the signal with a simple gaussian function while we want to model data from MJD accounting for an additional tail at the low-energy side of the gaussian. See the \"Partitions and events\" documentation.","category":"page"},{"location":".ipynb_checkpoints/index-checkpoint/#ZeroNuFit.jl-Documentation","page":"ZeroNuFit.jl Documentation","title":"ZeroNuFit.jl Documentation","text":"","category":"section"},{"location":".ipynb_checkpoints/index-checkpoint/","page":"ZeroNuFit.jl Documentation","title":"ZeroNuFit.jl Documentation","text":"Welcome to the documentation for ZeroNuFit.jl.","category":"page"},{"location":".ipynb_checkpoints/index-checkpoint/#Introduction","page":"ZeroNuFit.jl Documentation","title":"Introduction","text":"","category":"section"},{"location":".ipynb_checkpoints/index-checkpoint/","page":"ZeroNuFit.jl Documentation","title":"ZeroNuFit.jl Documentation","text":"ZeroNuFit.jl is a Julia package for running an extended unbinned fit of a Gaussian signal over a background for the neutrinoless double-beta decay (0nubetabeta) analysis. The tool was developed for the LEGEND experiment but it can be easily extended to data collected by any other 0nubetabeta experiment or to any other physical processes that can be modelled in a similar way.","category":"page"},{"location":".ipynb_checkpoints/index-checkpoint/","page":"ZeroNuFit.jl Documentation","title":"ZeroNuFit.jl Documentation","text":"The package uses the BAT.jl tool box  for Bayesian inference (see O. Schulz, F. Beaujean, A. Caldwell, C. Grunwald, V. Hafych, K. Kröninger et al., “Bat.jl: A julia-based tool for bayesian inference”, SN Computer Science 2 (2021) 210).","category":"page"},{"location":".ipynb_checkpoints/index-checkpoint/#Table-of-contents","page":"ZeroNuFit.jl Documentation","title":"Table of contents","text":"","category":"section"},{"location":".ipynb_checkpoints/index-checkpoint/","page":"ZeroNuFit.jl Documentation","title":"ZeroNuFit.jl Documentation","text":"Pages = [\n    \"likelihood.md\",\n    \"installation.md\",\n    \"config.md\",\n    \"inputs.md\",\n    \"toys.md\",\n    \"tutorial.md\",\n    \"api.md\",\n]\nDepth = 1","category":"page"},{"location":".ipynb_checkpoints/installation-checkpoint/#First-steps","page":"First steps","title":"First steps","text":"","category":"section"},{"location":".ipynb_checkpoints/installation-checkpoint/#How-to-run-the-code","page":"First steps","title":"How to run the code","text":"","category":"section"},{"location":".ipynb_checkpoints/installation-checkpoint/","page":"First steps","title":"First steps","text":"Run the following command by specifying the path to the configuration file used for settings:","category":"page"},{"location":".ipynb_checkpoints/installation-checkpoint/","page":"First steps","title":"First steps","text":"$ julia main.jl -c config/config.json","category":"page"},{"location":".ipynb_checkpoints/installation-checkpoint/#Julia-Project-enviroments","page":"First steps","title":"Julia Project enviroments","text":"","category":"section"},{"location":".ipynb_checkpoints/installation-checkpoint/","page":"First steps","title":"First steps","text":"To run the code in a virtual enviroment you can use the following. Start by entering the Julia command view by typing julia on your terminal. Once inside, enter the packagge manager (via ]), activate the environment in the current directory, and resolve (and install, if necessary) dependencies:","category":"page"},{"location":".ipynb_checkpoints/installation-checkpoint/","page":"First steps","title":"First steps","text":"pkg> ] \npkg> activate .\npkg> instantiate","category":"page"},{"location":".ipynb_checkpoints/installation-checkpoint/","page":"First steps","title":"First steps","text":"Now you can run the script inside this enviroment with:","category":"page"},{"location":".ipynb_checkpoints/installation-checkpoint/","page":"First steps","title":"First steps","text":"$ julia main.jl --project=. -c config/config.json","category":"page"},{"location":"toys/#Generating-toys","page":"Generating toys","title":"Generating toys","text":"","category":"section"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"Table of contents:","category":"page"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"Pages = [\"toys.md\"]\nDepth = 3","category":"page"},{"location":"toys/#Posterior-predictive-distribution-studies","page":"Generating toys","title":"Posterior predictive distribution studies","text":"","category":"section"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"In order to check for any mis-modeling in the fits and to further understand the derived half-life limits, we performed \"sensitivity\" studies.  In a Bayesian context, the sensitivity is related to the concept of posterior predictive distributions. The prior predictive distribution is the expected distribution of data coming from a future experiment identical to that performed and repeated under the same conditions. This marginalizes the uncertainty in the model parameters, based on their prior distributions.  Alternatively, the posterior predictive distribution weights the data by the posterior obtained from the analysis. If the original data were modeled appropriately, then fake data generated under the given model should distribute similarly to the original data (see A. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin, \"Bayesian Data Analysis\", 3rd ed. 798, Chapman & Hall, 2013 for further details).","category":"page"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"Considering the observed signal counts as an observable of the data, we can thus extract \"sensitivity curves\" that can be interpreted as the distribution of expected future limits derived from repeating the identical experiment. This is distinct from a frequentist sensitivity since uncertainty on nuisance parameters are marginalized over.","category":"page"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"Sensitivity studies can be run over N_rm tot toy spectra generated by sampling the posterior marginalized distributions of nuisance parameters obtained in the no-signal hypothesis fit, i.e. in a fit where the signal contribution was excluded from the modeling of events.  In particular, starting from the posterior distributions derived for all nuisance parameters via the fit over original data, p(theta  D), data were sampled from a model's posterior distribution as follows:","category":"page"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"beginaligned\n    p(y  D) = int p(y  theta) p(theta  D) dtheta\nendaligned","category":"page"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"where y are the generated data, D are the observed data, and theta are the model parameters (but the signal).  This is achieved by drawing samples of theta from the posterior distribution p(theta  D), and subsequently generating datasets y based on the likelihood model p(y  theta).  In particular, for each partition, the expected number of background events was calculated using Poisson sampling, n_rm b sim textPois(mu_b), so that n_rm b events could be uniformly generated in the considered fit energy window to build a toy energy spectrum. Each generated toy energy spectrum can be later fitted with a signal+background model.","category":"page"},{"location":"toys/#Running-toys","page":"Generating toys","title":"Running toys","text":"","category":"section"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"A module sensitivity.jl is present for generating toys and running sensitivity studies. The script can be run as","category":"page"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"$ julia sensitivity.jl -c config_fake_data.json -i N","category":"page"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"where N=1N_rm tot is an integer number corresponding to the toy index. The input config file (config_fake_data.json) has the following entries:","category":"page"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"{\n    \"path_to_fit\": \"output/fit_gerda_phIandphII_NoSignal/\",\n    \"best_fit\": false,\n    \"low_stat\": true,\n    \"seed\": null\n}\n","category":"page"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"where","category":"page"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"\"path_to_fit\" is the path to the already performed fit over real data;\n\"best_fit\": true if we want to fix the paramaters to the best fit coming from the fit under the no-signal hypothesis;\n\"low_stat\": true to run 5 MCMC chains with 10^5 steps each; if false, then the statistics used for the original fit under the no-signal hypothesis is used;\n\"seed\": null if we want a random seed when generating fake data, otherwise you can fix it to an Int value.","category":"page"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"Below, we show an example of a bash script used for running sensitivity studies as multiple jobs on NERSC:","category":"page"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"#!/bin/bash                                                                                                                                                 \n#SBATCH -q regular                                                                                                                                       \n#SBATCH --constraint=cpu                                                                                                                                    \n#SBATCH -t 48:00:00\n#SBATCH -J sens_test                                                                                                                                         \n#SBATCH --output parallel.log                                                     \n#SBATCH --error parallel.err  \n\nmodule load parallel\nmodule load julia\nsrun=\"srun -N 1\"\nparallel=\"parallel --delay 1 -j 128\"\n\n# run parallel jobs\n$srun  $parallel \"julia sensitivity.jl -c config_fake_data.json -i {1}\" ::: {1..10000} &\n\nwait","category":"page"},{"location":"toys/#Testing-alternative-models-using-already-existing-toys","page":"Generating toys","title":"Testing alternative models using already existing toys","text":"","category":"section"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"Another way to run the code is present if, for instance, an user wants to use toy data generated according to one model and fit them with a different model. In this case, the path to the folder containing the already existing JSON files with toy energy events previously generated has to be provided together with the toy index:","category":"page"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"julia sensitivity.jl -c config_fake_data.json -i N --path_to_toys path_to_your_toys","category":"page"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"Below, an example of a bash script that can be used for retrieving multiple existing toy data and running sensitivity studies as multiple jobs on NERSC:","category":"page"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"#!/bin/bash                                                                                                                                                 \n#SBATCH -q regular                                                                                                                                       \n#SBATCH --constraint=cpu                                                                                                                                    \n#SBATCH -t 48:00:00\n#SBATCH -J sens_test                                                                                                                                         \n#SBATCH --output parallel.log                                                     \n#SBATCH --error parallel.err             \n\n# set the directory path to toys\npath_to_toys=\"path_to_your_toys\"\nall_files=(\"$path_to_toys\"/*.json)\nfull_paths=()\nfor file in \"${all_files[@]}\"; do\n    if [[ -f \"$file\" ]]; then \n        full_paths+=(\"$file\")\n    fi\ndone\nif [ ${#full_paths[@]} -eq 0 ]; then\n    echo \"The list of existing toy data is empty! Exit here.\"\n    exit 1\nelse\n    echo \"You are going to run a fit over ${#full_paths[@]} number of already existing toys stored under $path_to_toys\"\nfi\n\n# array to hold toy_idx\ntoy_indices=()\n\n# loop over available fake JSON toys\nfor path in \"${full_paths[@]}\"; do\n    base_name=\"${path%.json}\"\n    number_str=\"${base_name##*fake_data}\"  \n    toy_idx=$((number_str))  \n\n    toy_indices+=(\"$toy_idx\") \ndone\necho \"List of toy indices: ${toy_indices[*]}\"\n\nmodule load parallel\nmodule load julia\nsrun=\"srun -N 1\"\nparallel=\"parallel --delay 1 -j 128\"\n$srun $parallel \"julia sensitivity.jl -c config/toy_9_l200_1BI_new_data_same_bkg_noS.json -i {1}\" ::: \"${toy_indices[@]}\"\n\nwait","category":"page"},{"location":"toys/#Testing-fake-scanerios","page":"Generating toys","title":"Testing fake scanerios","text":"","category":"section"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"In the above examples, you can replace the julia-running line in order to test the \"sensitivity\" of the experiment for fixed input partitions parameters and a fixed level of background (<bkg_index>: expressed in units of 10^-4 counts/keV/kg/yr) for the exclusion scenario:","category":"page"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"$srun  $parallel \"julia sensitivity.jl -c config/fake_config.json -i {1} -f true -b <bkg_idex>\" ::: {1..10000} ","category":"page"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"An example of fake_partitions.json` input is the following, where you fill entries with the values you want to test the 0nubetabeta sensitivity:","category":"page"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"{\n    \"fit_groups\": {\n        \"all_l200a\": {\n            \"range\": [\n                [1930.0, 2098.511], [2108.511, 2113.513], [2123.513, 2190.0}\n            ],\n            \"model\": \"uniform\",\n            \"bkg_name\": \"B_l200a_all\"\n        }\n    },\n    \"partitions\": {\n        \"all_l200a\": [\n            {\n            \"experiment\": \"L200\",\n            \"detector\": \"FAKEDET\", // random name\n            \"part_name\": \"part0001\", // partition ID\n            \"start_ts\": 1704950367, // used to map the enregy events to the correct partition ID\n            \"end_ts\": 1708271505, // used to map the enregy events to the correct partition ID\n            \"eff_tot\": 0.69,\n            \"eff_tot_sigma\": 0, \n            \"width\": 1.0616522503600239, // energy resolution in standard deviation (in keV)\n            \"width_sigma\": 0,\n            \"fwhm\": 2.5, // energy resolution in FWHM (in keV)\n            \"fwhm_sigma\": 0,\n            \"exposure\": 1000,\n            \"bias\": 0, \n            \"bias_sigma\": 0\n            }\n        ]\n    }\n}","category":"page"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"The code works for deriving the exclusion sensitivity, so for the fake_events.json input just use:","category":"page"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"{\n    \"events\": [\n    ]\n}","category":"page"},{"location":"toys/","page":"Generating toys","title":"Generating toys","text":"The implementation of a discovery sensitivty study has not been carried on.","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/#Configuration-file","page":"Configuration file","title":"Configuration file","text":"","category":"section"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"Table of contents:","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"Pages = [\"config.md\"]\nDepth = 3","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/#Building-the-configuration-file","page":"Configuration file","title":"Building the configuration file","text":"","category":"section"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"Before running the code, set the input config.json file with following entries:","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"{\n    \"debug\":false,\n    \"partitions\":[\"config/partitions_gerda_new.json\",\"config/partitions_l200.json\",\"config/partitions_mjd_new.json\"],\n    \"events\":    [\"config/events_gerda.json\",\"config/events_l200.json\",\"config/events_mjd_new_part.json\"],\n    \"output_path\": \"output/fit_mjd_l200_gerda_v2/\",\n    \"overwrite\":true,\n    \"bat_fit\": {\"nchains\": 6, \"nsteps\": 1e6},\n    \"plot\": {\n            \"fit_and_data\": false,\n            \"bandfit_and_data\": false,\n            \"scheme\":\"red\",\n            \"alpha\":0.3\n        },\n    \"bkg_only\": false,\n    \"signal\": {\"upper_bound\":1000, \"prior\": \"uniform\"},\n    \"bkg\": {\n             \"units\": \"ckky\",\n             \"upper_bound\":0.1,\n             \"prior\": \"uniform\",\n             \"correlated\": {\"mode\": \"none\", \"range\": \"none\"}\n             },\n    ...\n}","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"where","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"\"debug\": true if you want to display debug output on terminal;\n\"partitions\": list of partitions JSON inputs; it takes one entry per experiment;\n\"events\": list of events JSON inputs; it takes one entry per experiment;\n\"output_path\": path where to store outputs (logs, plots, mcmc results);\n\"overwrite\": true if you want to overwrite a previous fit with same output_path; if set to false but no fits were previously performed (ie there are no outputs to overwrite), the code will save the output of this fit;\n\"bat_fit\": settings for the BAT fit, \"nchains\" numer of MCMC chains to run, \"nsteps\" number of steps for each MCMC chain;\n\"plot\": settings for plotting; \"fit_and_data\": true plots fit line over data (and CI bands if \"bandfit_and_data\": true); \"scheme\":\"red\" and \"alpha\":0.3 are used for customizing output appearances;\n\"bkg_only\": true if we fit assuming no signal (S=0), false otherwise;\n\"signal\": select \"upper_bound\" for the prior and the \"prior\" shape (uniform, sqrt, ...);\n\"bkg\": select \"upper_bound\" for the prior range and the \"prior\" shape (uniform, ...). There are several optional keys with details given below: if these are not provided, the fit defaults to a flat background without correlations. Notice the units of the final BI can be chosen among \"ckky\" (counts/keV/kg/yr) or \"cFty\" (counts/FWHM/t/yr).","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"Moreover, the config requires the following block for nuisance parameters, ie energy scale (=energy bias and resolution) and efficiency:","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"    {\n    ...\n    \"nuisance\": { \n         \"energy_bias\": {\n            \"fixed\": false,\n            \"correlated\": false\n         },\n         \"energy_res\": {\n            \"fixed\": false,\n            \"correlated\": false\n         },\n         \"efficiency\" : {\n            \"correlated\": true,\n            \"fixed\": false\n            }\n    }","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"In particular, you can set \"correlated\": true if you want to use one variable to correlate the nuisance parameters (eg to speed up the computation times), and \"fixed\": false if you want to include a prior for nuisance parameters (otherwise these parameters they will be fixed to their partition value and not constrained).","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"If a variable is correlated (either energy_bias or energy_res or efficiency), the code will search for a field in the fit_groups block of the partitions JSON file to use a correlated variable per each fit group.  In particular, the field can be specified as:","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"\"energy_bias_group_name\": \"...\"\n\"energy_reso_group_name\": \"...\"\n\"efficiency_group_name\": \"...\"","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"Parameters are then added to the model called αr_\\$name (for resolution), αe_\\$name for efficiency and αb_\\$name for bias.","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"warning: Warning\nThe alpha parameter names default to _all, if you want one different per experiment this must be explicitly specified in the fit groups entry","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/#Background-shape-and-correlation","page":"Configuration file","title":"Background shape and correlation","text":"","category":"section"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"There are several options to control the background in more detail. These can be added to the \"bkg\" part of the config: In particular:","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"\"correlated\" adds a hierarchical (correlated) background to the model, this key should have a dictionary giving details on the prior shape and ranges. The three options for the mode are: none (default), lognormal or normal. The range entry is associated to the range used for the uniform prior on the \\$sigma_B parameter. For example:","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"\"correlated\": {\"mode\":\"lognormal\",\"range\":[0,1]}","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"\"shape\" changes the shape of the background from uniform. The user should provide a dictionary giving details on the shape. For example:","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"\"shape\": {\n            \"name\":\"exponential\",\n            \"pars\":{\"slope\":[-10,10]}\n        },","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"The \"pars\" subdictionary describes the range of the priors on the parameters of the model, currently implemented shapes are \"uniform\", \"linear\" and \"exponential\". These names correspond to functions in fitting.jl and logical conditions in get_bkg_pdf in likelihood.jl.","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"This will add parameters ${bkg_name}_slope or similar to the model (and then call them). This names therefore must correspond to the names in the functions in fitting.jl. To add a new shape simply define a new method in fitting.jl and a new logical condition in get_bkg_pdf in likelihood.jl.","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"note: Note\nIf these keys are not provided the model defaults to a uniform uncorrelated background.","category":"page"},{"location":".ipynb_checkpoints/config-checkpoint/#Signal-shape","page":"Configuration file","title":"Signal shape","text":"","category":"section"},{"location":".ipynb_checkpoints/config-checkpoint/","page":"Configuration file","title":"Configuration file","text":"The signal shape can be specified in the partition JSON file in order to have a different signal shape for different partitions. An example is given by the combined fit of GERDA/LEGEND and MAJORANA DEMONSTRATOR (MJD) partitions where for the first one we want to model the signal with a simple gaussian function while we want to model data from MJD accounting for an additional tail at the low-energy side of the gaussian. See the \"Partitions and events\" documentation.","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/#Likelihood-implementation","page":"Likelihood implementation","title":"Likelihood implementation","text":"","category":"section"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"Table of contents:","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"Pages = [\"likelihood.md\"]\nDepth = 3","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/#Likelihood-Function","page":"Likelihood implementation","title":"Likelihood Function","text":"","category":"section"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"The implemented unbinned Likelihood function reads as: ","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n    mathcalL(Gamma boldsymbolBIboldsymbolthetaD) = prod_k bigg textrmPois(s_k+b_k) bigg prod_i_k=1^N_k frac1s_k + b_k left( b_kcdot p_rm b(E) + s_rm kcdot p_rm s(E) right)  bigg bigg\nendaligned","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"where Gamma is the signal rate, BI is the background index, boldsymboltheta are the nuisance parameters, and D are the observed data. Here, the first product runs over the number of partitions k (N_rm p partitions in total) and the second over the events i in a given partition (N_rm k events in total). In case no events are found in a given partition k, the above Likelihood expression simplifies into","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n    mathcalL(Gamma boldsymbolBIboldsymbolthetaD) = prod_k textrmPois(s_k+b_k) \nendaligned","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"The terms p_rm b(E) and p_rm s(E) represent the background and signal distributions, respectively, both expressed as a function of energy E. The background distribution can be selected among different options: flat, linear or exponential. The signal distribution can be expressed in the following way","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n    p_rm s(E) = fracdP(E_rm i  Q_betabeta - Delta_rm k omega_rm k)dE \nendaligned","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"for a given energy E_rm i of an event falling in the analysis window for the partition k with energy bias Delta_rm k and energy width omega_rm k. Here, we are assuming the energy biases were defined as E_rm true - E_rm cal. Thus, we correct for the energy bias by adding the calculated bias to the calibrated event energy.","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"Taking x=Q_betabeta - Delta_rm k, the signal energy distribution for each partition can be taken as a Gaussian (e.g. for GERDA/LEGEND), ","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\nfracdP(E  x sigma)dE = frac1sqrt2pisigma^2 times e^-fracleft(E - x right)^22 sigma ^2\nendaligned","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"Alternatively, the signal energy distribution can also be shaped as a Gaussian with a tail at low energies (e.g. for MAJORANA DEMONSTRATOR data), ","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\nfracdP(E  x gamma)dE = frac1-fsqrt2pi(gamma sigma)^2 times e^-fracleft(E - x right)^22 (gamma sigma) ^2+fracf2gammatautimes e^ frac(gamma sigma)^22(gammatau)^2 + fracE-xgammatau  times erfc left(fracsigmasqrt2tau+ fracE-xsqrt2gammasigma right)\nendaligned","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"where f is the fraction of events in the tail (taken as fixed), tau is the scale parameter of the tail. The gamma parameter correlates the uncertainties on both sigma and tau by simultaneously scaling sigma and tau with nominal value tildegamma = 1 and uncertainty delta_gamma. Therefore, sigma and tau are taken as fixed parameters and all uncertainty is handled by gamma.","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"The background and signal counts, i.e. b_rm k and s_rm k respectively, are defined as","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\nb_rm k = mathcalB_rm b cdot Delta E cdot mathcalE_rm k\nendaligned","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"and","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\ns_rm k = fractextln2cdot mathcalN_rm Am_rm 76 cdot (varepsilon_rm k + alpha cdot sigma_varepsilon_rm k) cdot mathcalE_rm k cdot Gamma\nendaligned","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"Notice that an extra index b was introduced to account for different background indexes mathcalB_rm b that might be shared across different partitions. In particular, for a given partition k, mathcalE_rm k is the exposure, Delta E is the net width of the fit window, varepsilon_rm k is the efficiency with uncertainty sigma_varepsilon_rm k and Gamma is the signal rate.","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/#Likelihood-implementation-2","page":"Likelihood implementation","title":"Likelihood implementation","text":"","category":"section"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"In our framework, the above Likelihood product was evaluated taking the logarithm of it.  We defined our \"log Likelihood\" (LL) as:","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n    LL(Gamma)  = underbracesum_rm jsum_rm i_rm k=1^N_rm k lefttextlogleft(Pois(b_rm j+s_rm j)right) + textlogleft(b_rm j cdot p_rm b(E) + s_rm j cdot p_rm s(E) right) - textlogleft(b_rm j+s_rm j right) right_N_rm ktext events in j - underbracesum_l left(b_l+s_l right)_text0 events in l\nendaligned","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"The sum over all partitions k was separated in a sum over partitions containing an event i with energy E_rm i (sum with index j) and in a sum over partitions with no events (sum with index l).","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/#Prior-terms","page":"Likelihood implementation","title":"Prior terms","text":"","category":"section"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"Different free prameters can be identified within the framework:","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"signal, Gamma\nbackground indeces, mathcalB_rm b (with b=1N)\nenergy bias at Q_betabeta, Delta = E_textrmtrue - E_textrmcal (keV)\nenergy resolution, sigma (keV)\npeak shape scale parameter, gamma (for the modified Gaussian peak shape only)\nsignal efficiencies, varepsilon","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"For the signal prior, either a uniform or a 1sqrtGamma prior can be used.","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"For the background indeces, a uniform prior can be used.","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"For nuisance parameters theta (energy biases, energy resolutions and efficiencies), Gaussian distributions centred around the true values were used","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n    mathcalP(boldsymboltheta) =mathcalP(alpha) cdot  prod_rm k mathcalP_rm k(Delta)cdot mathcalP_rm k(sigma) =  e^-alpha^22 cdot prod_rm k frac1sqrt2pisigma_Delta_rm k \n      e^-fracleft(Delta_rm k - widehatDelta_rm k  right)^22 sigma_Delta_rm k ^2 cdot\n      frac1sqrt2pisigma_sigma_rm k \n      e^-fracleft(sigma_rm k - widehatsigma_rm k  right)^22 sigma_sigma_rm k ^2\nendaligned","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"Alternatively, the code allows the user to fix energy biases, energy resolutions and efficiencies at their true values, removing any prior constraint. Another feature of the framework includes the possibility to correlate energy biases and resolutions via a unique term, alpha_rm bias or alpha_rm reso, as it is done for the efficiencies in the above case. Indeed, alpha is used as a scaling parameter that fully correlated the efficiency uncertainties across all partitions, with nominal value of 0 and standard deviation of 1. Separating each partition efficiency into uncorrelated components and adding a nuisance parameter for each would require a great deal of CPU. ","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"Accounting for the different signal shape (i.e. modified Gaussian with a tail at low energies), we includeed a Gaussian prior term for the peak position offset (mu) and the peak shape scale parameter (gamma):","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n      mathcalP_rm mod(boldsymboltheta) =mathcalP(alpha) cdot   prod_rm k mathcalP_rm k(mu)cdot mathcalP_rm k(sigma)=e^-alpha^22 cdotprod_rm k  frac1sqrt2pisigma_mu_rm k \n      e^-fracleft(mu_rm k - widehatmu_rm k  right)^22 sigma_mu_rm k ^2 cdot\n      frac1sqrt2pisigma_gamma_rm k \n      e^-fracleft(gamma_rm k - widehatgamma_rm k  right)^22 sigma_gamma_rm k ^2  \nendaligned","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"In the code, the product of priors was further simplified by introducing a prior term only for those partitions j for which there is at least an event.  The above products, then, can be expressed again as","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n    mathcalP(boldsymboltheta) =mathcalP(alpha) cdot  prod_rm j mathcalP_rm j(Delta)cdot mathcalP_rm j(sigma) text and \n    mathcalP_rm mod(boldsymboltheta) =mathcalP(alpha) cdot  prod_rm j mathcalP_rm j(mu)cdot mathcalP_rm j(sigma)\nendaligned","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/#Alternative-background-shapes","page":"Likelihood implementation","title":"Alternative background shapes","text":"","category":"section"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"The default background modeling shape is a flat function:","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n  f_rm flat(E) = b_rm k cdot p_rm bflat(E) = BI cdot Delta E cdot mathcalE_rm k cdot underbracefrac1K_rm flat_p_rm bflat(E)\nendaligned","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"where E is the energy and K_rm flat is a normalization factor that accounts for the net width of the fit window, accounting for any removed gap within it.","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"In the linear case, we can model the background as","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n  f_rm lin(E) = b_rm k cdot p_rm blin(E) = BI cdot Delta E cdot mathcalE_rm k cdot underbraceleft(1+ fracm_rm lin cdot (E-E_rm 0)E_rm max-E_rm 0 right)cdot frac1K_rm lin_p_rm blin(E)\nendaligned","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"where m_rm lin is the slope of the linear function, E_rm 0 (E_rm max) is the starting (ending) energy value of the fit window, and K_rm lin is the normalization factor.","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"In the exponential case, we can model the background as","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n  f_rm exp(E) = b_rm k cdot p_rm bexp(E) = BI cdot Delta E cdot mathcalE_rm k cdot underbracee ^ left(E-E_rm 0right)cdot fracm_rm expDelta E  cdot frac1K_rm exp_p_rm bexp(E) \nendaligned","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"with corresponding slope m_rm exp and normalization factor K_rm exp.","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"The normalization factors can be expressed in a general form in the following way:","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n    begincases\n      K_rm flat = Delta E15pt\n      K_rm lin = Delta E cdot left(1 - fracm_rm lincdot E_rm 0E_rm max-E_rm 0 right) + m_rm lincdot fracsum_i left( E_rm hi^2 - E_rm li^2 right)2left(E_rm max-E_rm 0 right )15pt\n      K_rm exp = fracE_rm max-E_rm 0m_rm exp cdot leftsum_rm ie^(E_rm hi-E_rm 0)cdot fracm_rm expE_rm max-E_rm 0 - sum_rm ie^(E_rm li-E_rm 0)cdot fracm_rm expE_rm max-E_rm 0 right\n    endcases  \nendaligned","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"where E_rm li (E_rm hi) is the starting (ending) energy value of the sub-windows defined within the analysis fit window, accounting for any excluded gap in the fit window.  If no gaps are present, then E_rm liequiv E_rm 0 and E_rm hiequiv E_rm max. In particular, Delta E= sum_rm i left(E_rm hi-E_rm li right).","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"Notice that K_rm lin rightarrow K_rm flat and K_rm exp rightarrow K_rm flat in the limit of m_rm lin rightarrow 0 and m_rm exp rightarrow 0, respectively.  The free parameters in the linear and exponential case are m_rm lin and m_rm exp (other than the background indices, BI). For all these parameters, we used a uniform prior as default, while prior ranges can be modified by the user via the configuration file. ","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/#Posterior-distributions-and-marginalization","page":"Likelihood implementation","title":"Posterior distributions and marginalization","text":"","category":"section"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"The combined posterior probability density function is calculated according to Bayes’ theorem as:","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n    mathcalP(Gamma boldsymbolBI boldsymbolthetaD ) propto underbracemathcalL(Gamma boldsymbolBIboldsymbolthetaD)_textLikelihood cdot underbracemathcalP(boldsymboltheta) cdot mathcalP(Gamma)cdot prod_rm b=1^N_rm b mathcalP_rm b(BI)_textprior terms\nendaligned","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"where D are our data, i.e. the energy events surviving all cuts in the fit window.  Here, we expressed the general case where a total number of N_rm b BIs are introduced. ","category":"page"},{"location":".ipynb_checkpoints/likelihood-checkpoint/","page":"Likelihood implementation","title":"Likelihood implementation","text":"The marginalization is performed with the BAT toolkit via a Markov chain Monte Carlo (MCMC) numerical integration. The marginalization used the Metropolis-Hastings sampling algorithm implemented in BAT.  The number of MCMC chains and the number of steps in each MCMC can be selected by the user.","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/#Generating-toys","page":"Generating toys","title":"Generating toys","text":"","category":"section"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"Table of contents:","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"Pages = [\"toys.md\"]\nDepth = 3","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/#Posterior-predictive-distribution-studies","page":"Generating toys","title":"Posterior predictive distribution studies","text":"","category":"section"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"In order to check for any mis-modeling in the fits and to further understand the derived half-life limits, we performed \"sensitivity\" studies.  In a Bayesian context, the sensitivity is related to the concept of posterior predictive distributions. The prior predictive distribution is the expected distribution of data coming from a future experiment identical to that performed and repeated under the same conditions. This marginalizes the uncertainty in the model parameters, based on their prior distributions.  Alternatively, the posterior predictive distribution weights the data by the posterior obtained from the analysis. If the original data were modeled appropriately, then fake data generated under the given model should distribute similarly to the original data (see A. Gelman, J. B. Carlin, H. S. Stern, D. B. Dunson, A. Vehtari, and D. B. Rubin, \"Bayesian Data Analysis\", 3rd ed. 798, Chapman & Hall, 2013 for further details).","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"Considering the observed signal counts as an observable of the data, we can thus extract \"sensitivity curves\" that can be interpreted as the distribution of expected future limits derived from repeating the identical experiment. This is distinct from a frequentist sensitivity since uncertainty on nuisance parameters are marginalized over.","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"Sensitivity studies can be run over N_rm tot toy spectra generated by sampling the posterior marginalized distributions of nuisance parameters obtained in the no-signal hypothesis fit, i.e. in a fit where the signal contribution was excluded from the modeling of events.  In particular, starting from the posterior distributions derived for all nuisance parameters via the fit over original data, p(theta  D), data were sampled from a model's posterior distribution as follows:","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"beginaligned\n    p(y  D) = int p(y  theta) p(theta  D) dtheta\nendaligned","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"where y are the generated data, D are the observed data, and theta are the model parameters (but the signal).  This is achieved by drawing samples of theta from the posterior distribution p(theta  D), and subsequently generating datasets y based on the likelihood model p(y  theta).  In particular, for each partition, the expected number of background events was calculated using Poisson sampling, n_rm b sim textPois(mu_b), so that n_rm b events could be uniformly generated in the considered fit energy window to build a toy energy spectrum. Each generated toy energy spectrum can be later fitted with a signal+background model.","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/#Running-toys","page":"Generating toys","title":"Running toys","text":"","category":"section"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"A module sensitivity.jl is present for generating toys and running sensitivity studies. The script can be run as","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"$ julia sensitivity.jl -c config_fake_data.json -i N","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"where N=1N_rm tot is an integer number corresponding to the toy index. The input config file (config_fake_data.json) has the following entries:","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"{\n    \"path_to_fit\": \"output/fit_gerda_phIandphII_NoSignal/\",\n    \"best_fit\": false,\n    \"low_stat\": true,\n    \"seed\": null\n}\n","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"where","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"\"path_to_fit\" is the path to the already performed fit over real data;\n\"best_fit\": true if we want to fix the paramaters to the best fit coming from the fit under the no-signal hypothesis;\n\"low_stat\": true to run 5 MCMC chains with 10^5 steps each; if false, then the statistics used for the original fit under the no-signal hypothesis is used;\n\"seed\": null if we want a random seed when generating fake data, otherwise you can fix it to an Int value.","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"Below, we show an example of a bash script used for running sensitivity studies as multiple jobs on NERSC:","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"#!/bin/bash                                                                                                                                                 \n#SBATCH -q regular                                                                                                                                       \n#SBATCH --constraint=cpu                                                                                                                                    \n#SBATCH -t 48:00:00\n#SBATCH -J sens_test                                                                                                                                         \n#SBATCH --output parallel.log                                                     \n#SBATCH --error parallel.err  \n\nmodule load parallel\nmodule load julia\nsrun=\"srun -N 1\"\nparallel=\"parallel --delay 1 -j 128\"\n\n# run parallel jobs\n$srun  $parallel \"julia sensitivity.jl -c config_fake_data.json -i {1}\" ::: {1..10000} &\n\nwait","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/#Testing-alternative-models-using-already-existing-toys","page":"Generating toys","title":"Testing alternative models using already existing toys","text":"","category":"section"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"Another way to run the code is present if, for instance, an user wants to use toy data generated according to one model and fit them with a different model. In this case, the path to the folder containing the already existing JSON files with toy energy events previously generated has to be provided together with the toy index:","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"julia sensitivity.jl -c config_fake_data.json -i N --path_to_toys path_to_your_toys","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"Below, an example of a bash script that can be used for retrieving multiple existing toy data and running sensitivity studies as multiple jobs on NERSC:","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"#!/bin/bash                                                                                                                                                 \n#SBATCH -q regular                                                                                                                                       \n#SBATCH --constraint=cpu                                                                                                                                    \n#SBATCH -t 48:00:00\n#SBATCH -J sens_test                                                                                                                                         \n#SBATCH --output parallel.log                                                     \n#SBATCH --error parallel.err             \n\n# set the directory path to toys\npath_to_toys=\"path_to_your_toys\"\nall_files=(\"$path_to_toys\"/*.json)\nfull_paths=()\nfor file in \"${all_files[@]}\"; do\n    if [[ -f \"$file\" ]]; then \n        full_paths+=(\"$file\")\n    fi\ndone\nif [ ${#full_paths[@]} -eq 0 ]; then\n    echo \"The list of existing toy data is empty! Exit here.\"\n    exit 1\nelse\n    echo \"You are going to run a fit over ${#full_paths[@]} number of already existing toys stored under $path_to_toys\"\nfi\n\n# array to hold toy_idx\ntoy_indices=()\n\n# loop over available fake JSON toys\nfor path in \"${full_paths[@]}\"; do\n    base_name=\"${path%.json}\"\n    number_str=\"${base_name##*fake_data}\"  \n    toy_idx=$((number_str))  \n\n    toy_indices+=(\"$toy_idx\") \ndone\necho \"List of toy indices: ${toy_indices[*]}\"\n\nmodule load parallel\nmodule load julia\nsrun=\"srun -N 1\"\nparallel=\"parallel --delay 1 -j 128\"\n$srun $parallel \"julia sensitivity.jl -c config/toy_9_l200_1BI_new_data_same_bkg_noS.json -i {1}\" ::: \"${toy_indices[@]}\"\n\nwait","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/#Testing-fake-scanerios","page":"Generating toys","title":"Testing fake scanerios","text":"","category":"section"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"In the above examples, you can replace the julia-running line in order to test the \"sensitivity\" of the experiment for fixed input partitions parameters and a fixed level of background (<bkg_index>: expressed in units of 10^-4 counts/keV/kg/yr) for the exclusion scenario:","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"$srun  $parallel \"julia sensitivity.jl -c config/fake_config.json -i {1} -f true -b <bkg_idex>\" ::: {1..10000} ","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"An example of fake_partitions.json` input is the following, where you fill entries with the values you want to test the 0nubetabeta sensitivity:","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"{\n    \"fit_groups\": {\n        \"all_l200a\": {\n            \"range\": [\n                [1930.0, 2098.511], [2108.511, 2113.513], [2123.513, 2190.0}\n            ],\n            \"model\": \"uniform\",\n            \"bkg_name\": \"B_l200a_all\"\n        }\n    },\n    \"partitions\": {\n        \"all_l200a\": [\n            {\n            \"experiment\": \"L200\",\n            \"detector\": \"FAKEDET\", // random name\n            \"part_name\": \"part0001\", // partition ID\n            \"start_ts\": 1704950367, // used to map the enregy events to the correct partition ID\n            \"end_ts\": 1708271505, // used to map the enregy events to the correct partition ID\n            \"eff_tot\": 0.69,\n            \"eff_tot_sigma\": 0, \n            \"width\": 1.0616522503600239, // energy resolution in standard deviation (in keV)\n            \"width_sigma\": 0,\n            \"fwhm\": 2.5, // energy resolution in FWHM (in keV)\n            \"fwhm_sigma\": 0,\n            \"exposure\": 1000,\n            \"bias\": 0, \n            \"bias_sigma\": 0\n            }\n        ]\n    }\n}","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"The code works for deriving the exclusion sensitivity, so for the fake_events.json input just use:","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"{\n    \"events\": [\n    ]\n}","category":"page"},{"location":".ipynb_checkpoints/toys-checkpoint/","page":"Generating toys","title":"Generating toys","text":"The implementation of a discovery sensitivty study has not been carried on.","category":"page"},{"location":"likelihood/#Likelihood-implementation","page":"Likelihood implementation","title":"Likelihood implementation","text":"","category":"section"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"Table of contents:","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"Pages = [\"likelihood.md\"]\nDepth = 3","category":"page"},{"location":"likelihood/#Likelihood-Function","page":"Likelihood implementation","title":"Likelihood Function","text":"","category":"section"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"The implemented unbinned Likelihood function reads as: ","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n    mathcalL(Gamma boldsymbolBIboldsymbolthetaD) = prod_k bigg textrmPois(s_k+b_k) bigg prod_i_k=1^N_k frac1s_k + b_k left( b_kcdot p_rm b(E) + s_rm kcdot p_rm s(E) right)  bigg bigg\nendaligned","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"where Gamma is the signal rate, BI is the background index, boldsymboltheta are the nuisance parameters, and D are the observed data. Here, the first product runs over the number of partitions k (N_rm p partitions in total) and the second over the events i in a given partition (N_rm k events in total). In case no events are found in a given partition k, the above Likelihood expression simplifies into","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n    mathcalL(Gamma boldsymbolBIboldsymbolthetaD) = prod_k textrmPois(s_k+b_k) \nendaligned","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"The terms p_rm b(E) and p_rm s(E) represent the background and signal distributions, respectively, both expressed as a function of energy E. The background distribution can be selected among different options: flat, linear or exponential. The signal distribution can be expressed in the following way","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n    p_rm s(E) = fracdP(E_rm i  Q_betabeta - Delta_rm k omega_rm k)dE \nendaligned","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"for a given energy E_rm i of an event falling in the analysis window for the partition k with energy bias Delta_rm k and energy width omega_rm k. Here, we are assuming the energy biases were defined as E_rm true - E_rm cal. Thus, we correct for the energy bias by adding the calculated bias to the calibrated event energy.","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"Taking x=Q_betabeta - Delta_rm k, the signal energy distribution for each partition can be taken as a Gaussian (e.g. for GERDA/LEGEND), ","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\nfracdP(E  x sigma)dE = frac1sqrt2pisigma^2 times e^-fracleft(E - x right)^22 sigma ^2\nendaligned","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"Alternatively, the signal energy distribution can also be shaped as a Gaussian with a tail at low energies (e.g. for MAJORANA DEMONSTRATOR data), ","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\nfracdP(E  x gamma)dE = frac1-fsqrt2pi(gamma sigma)^2 times e^-fracleft(E - x right)^22 (gamma sigma) ^2+fracf2gammatautimes e^ frac(gamma sigma)^22(gammatau)^2 + fracE-xgammatau  times erfc left(fracsigmasqrt2tau+ fracE-xsqrt2gammasigma right)\nendaligned","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"where f is the fraction of events in the tail (taken as fixed), tau is the scale parameter of the tail. The gamma parameter correlates the uncertainties on both sigma and tau by simultaneously scaling sigma and tau with nominal value tildegamma = 1 and uncertainty delta_gamma. Therefore, sigma and tau are taken as fixed parameters and all uncertainty is handled by gamma.","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"The background and signal counts, i.e. b_rm k and s_rm k respectively, are defined as","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\nb_rm k = mathcalB_rm b cdot Delta E cdot mathcalE_rm k\nendaligned","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"and","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\ns_rm k = fractextln2cdot mathcalN_rm Am_rm 76 cdot (varepsilon_rm k + alpha cdot sigma_varepsilon_rm k) cdot mathcalE_rm k cdot Gamma\nendaligned","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"Notice that an extra index b was introduced to account for different background indexes mathcalB_rm b that might be shared across different partitions. In particular, for a given partition k, mathcalE_rm k is the exposure, Delta E is the net width of the fit window, varepsilon_rm k is the efficiency with uncertainty sigma_varepsilon_rm k and Gamma is the signal rate.","category":"page"},{"location":"likelihood/#Likelihood-implementation-2","page":"Likelihood implementation","title":"Likelihood implementation","text":"","category":"section"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"In our framework, the above Likelihood product was evaluated taking the logarithm of it.  We defined our \"log Likelihood\" (LL) as:","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n    LL(Gamma)  = underbracesum_rm jsum_rm i_rm k=1^N_rm k lefttextlogleft(Pois(b_rm j+s_rm j)right) + textlogleft(b_rm j cdot p_rm b(E) + s_rm j cdot p_rm s(E) right) - textlogleft(b_rm j+s_rm j right) right_N_rm ktext events in j - underbracesum_l left(b_l+s_l right)_text0 events in l\nendaligned","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"The sum over all partitions k was separated in a sum over partitions containing an event i with energy E_rm i (sum with index j) and in a sum over partitions with no events (sum with index l).","category":"page"},{"location":"likelihood/#Prior-terms","page":"Likelihood implementation","title":"Prior terms","text":"","category":"section"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"Different free prameters can be identified within the framework:","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"signal, Gamma\nbackground indeces, mathcalB_rm b (with b=1N)\nenergy bias at Q_betabeta, Delta = E_textrmtrue - E_textrmcal (keV)\nenergy resolution, sigma (keV)\npeak shape scale parameter, gamma (for the modified Gaussian peak shape only)\nsignal efficiencies, varepsilon","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"For the signal prior, either a uniform or a 1sqrtGamma prior can be used.","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"For the background indeces, a uniform prior can be used.","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"For nuisance parameters theta (energy biases, energy resolutions and efficiencies), Gaussian distributions centred around the true values were used","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n    mathcalP(boldsymboltheta) =mathcalP(alpha) cdot  prod_rm k mathcalP_rm k(Delta)cdot mathcalP_rm k(sigma) =  e^-alpha^22 cdot prod_rm k frac1sqrt2pisigma_Delta_rm k \n      e^-fracleft(Delta_rm k - widehatDelta_rm k  right)^22 sigma_Delta_rm k ^2 cdot\n      frac1sqrt2pisigma_sigma_rm k \n      e^-fracleft(sigma_rm k - widehatsigma_rm k  right)^22 sigma_sigma_rm k ^2\nendaligned","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"Alternatively, the code allows the user to fix energy biases, energy resolutions and efficiencies at their true values, removing any prior constraint. Another feature of the framework includes the possibility to correlate energy biases and resolutions via a unique term, alpha_rm bias or alpha_rm reso, as it is done for the efficiencies in the above case. Indeed, alpha is used as a scaling parameter that fully correlated the efficiency uncertainties across all partitions, with nominal value of 0 and standard deviation of 1. Separating each partition efficiency into uncorrelated components and adding a nuisance parameter for each would require a great deal of CPU. ","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"Accounting for the different signal shape (i.e. modified Gaussian with a tail at low energies), we includeed a Gaussian prior term for the peak position offset (mu) and the peak shape scale parameter (gamma):","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n      mathcalP_rm mod(boldsymboltheta) =mathcalP(alpha) cdot   prod_rm k mathcalP_rm k(mu)cdot mathcalP_rm k(sigma)=e^-alpha^22 cdotprod_rm k  frac1sqrt2pisigma_mu_rm k \n      e^-fracleft(mu_rm k - widehatmu_rm k  right)^22 sigma_mu_rm k ^2 cdot\n      frac1sqrt2pisigma_gamma_rm k \n      e^-fracleft(gamma_rm k - widehatgamma_rm k  right)^22 sigma_gamma_rm k ^2  \nendaligned","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"In the code, the product of priors was further simplified by introducing a prior term only for those partitions j for which there is at least an event.  The above products, then, can be expressed again as","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n    mathcalP(boldsymboltheta) =mathcalP(alpha) cdot  prod_rm j mathcalP_rm j(Delta)cdot mathcalP_rm j(sigma) text and \n    mathcalP_rm mod(boldsymboltheta) =mathcalP(alpha) cdot  prod_rm j mathcalP_rm j(mu)cdot mathcalP_rm j(sigma)\nendaligned","category":"page"},{"location":"likelihood/#Alternative-background-shapes","page":"Likelihood implementation","title":"Alternative background shapes","text":"","category":"section"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"The default background modeling shape is a flat function:","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n  f_rm flat(E) = b_rm k cdot p_rm bflat(E) = BI cdot Delta E cdot mathcalE_rm k cdot underbracefrac1K_rm flat_p_rm bflat(E)\nendaligned","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"where E is the energy and K_rm flat is a normalization factor that accounts for the net width of the fit window, accounting for any removed gap within it.","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"In the linear case, we can model the background as","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n  f_rm lin(E) = b_rm k cdot p_rm blin(E) = BI cdot Delta E cdot mathcalE_rm k cdot underbraceleft(1+ fracm_rm lin cdot (E-E_rm 0)E_rm max-E_rm 0 right)cdot frac1K_rm lin_p_rm blin(E)\nendaligned","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"where m_rm lin is the slope of the linear function, E_rm 0 (E_rm max) is the starting (ending) energy value of the fit window, and K_rm lin is the normalization factor.","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"In the exponential case, we can model the background as","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n  f_rm exp(E) = b_rm k cdot p_rm bexp(E) = BI cdot Delta E cdot mathcalE_rm k cdot underbracee ^ left(E-E_rm 0right)cdot fracm_rm expDelta E  cdot frac1K_rm exp_p_rm bexp(E) \nendaligned","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"with corresponding slope m_rm exp and normalization factor K_rm exp.","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"The normalization factors can be expressed in a general form in the following way:","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n    begincases\n      K_rm flat = Delta E15pt\n      K_rm lin = Delta E cdot left(1 - fracm_rm lincdot E_rm 0E_rm max-E_rm 0 right) + m_rm lincdot fracsum_i left( E_rm hi^2 - E_rm li^2 right)2left(E_rm max-E_rm 0 right )15pt\n      K_rm exp = fracE_rm max-E_rm 0m_rm exp cdot leftsum_rm ie^(E_rm hi-E_rm 0)cdot fracm_rm expE_rm max-E_rm 0 - sum_rm ie^(E_rm li-E_rm 0)cdot fracm_rm expE_rm max-E_rm 0 right\n    endcases  \nendaligned","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"where E_rm li (E_rm hi) is the starting (ending) energy value of the sub-windows defined within the analysis fit window, accounting for any excluded gap in the fit window.  If no gaps are present, then E_rm liequiv E_rm 0 and E_rm hiequiv E_rm max. In particular, Delta E= sum_rm i left(E_rm hi-E_rm li right).","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"Notice that K_rm lin rightarrow K_rm flat and K_rm exp rightarrow K_rm flat in the limit of m_rm lin rightarrow 0 and m_rm exp rightarrow 0, respectively.  The free parameters in the linear and exponential case are m_rm lin and m_rm exp (other than the background indices, BI). For all these parameters, we used a uniform prior as default, while prior ranges can be modified by the user via the configuration file. ","category":"page"},{"location":"likelihood/#Posterior-distributions-and-marginalization","page":"Likelihood implementation","title":"Posterior distributions and marginalization","text":"","category":"section"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"The combined posterior probability density function is calculated according to Bayes’ theorem as:","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"beginaligned\n    mathcalP(Gamma boldsymbolBI boldsymbolthetaD ) propto underbracemathcalL(Gamma boldsymbolBIboldsymbolthetaD)_textLikelihood cdot underbracemathcalP(boldsymboltheta) cdot mathcalP(Gamma)cdot prod_rm b=1^N_rm b mathcalP_rm b(BI)_textprior terms\nendaligned","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"where D are our data, i.e. the energy events surviving all cuts in the fit window.  Here, we expressed the general case where a total number of N_rm b BIs are introduced. ","category":"page"},{"location":"likelihood/","page":"Likelihood implementation","title":"Likelihood implementation","text":"The marginalization is performed with the BAT toolkit via a Markov chain Monte Carlo (MCMC) numerical integration. The marginalization used the Metropolis-Hastings sampling algorithm implemented in BAT.  The number of MCMC chains and the number of steps in each MCMC can be selected by the user.","category":"page"},{"location":"#ZeroNuFit.jl-Documentation","page":"Home","title":"ZeroNuFit.jl Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Welcome to the documentation for ZeroNuFit.jl.","category":"page"},{"location":"#Introduction","page":"Home","title":"Introduction","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ZeroNuFit.jl is a Julia package for running an extended unbinned fit of a Gaussian signal over a background for the neutrinoless double-beta decay (0nubetabeta) analysis. The tool was developed for the LEGEND experiment but it can be easily extended to data collected by any other 0nubetabeta experiment or to any other physical processes that can be modelled in a similar way.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The package uses the BAT.jl tool box  for Bayesian inference (see O. Schulz, F. Beaujean, A. Caldwell, C. Grunwald, V. Hafych, K. Kröninger et al., “Bat.jl: A julia-based tool for bayesian inference”, SN Computer Science 2 (2021) 210).","category":"page"},{"location":"#Table-of-contents","page":"Home","title":"Table of contents","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"likelihood.md\",\n    \"installation.md\",\n    \"config.md\",\n    \"inputs.md\",\n    \"toys.md\",\n    \"tutorial.md\",\n    \"api.md\",\n]\nDepth = 1","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The aim of this tutorial consists in building proper config JSON files in order to run a neutrinoless double-beta decay analysis over GERDA and MAJORANA DEMONSTRATO (MJD) published data. Additional info on the meaning of input parameters can be found under the \"Configuration file\" section, and for input files under the \"Partitions and events\" section.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Table of contents:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Pages = [\"tutorial.md\"]\nDepth = 3","category":"page"},{"location":"tutorial/#GERDA-Phase-I:-SB-fit","page":"Tutorial","title":"GERDA Phase I: S+B fit","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let's start by fitting data acquired by the GERDA experiment during its Phase I and let's fit them with a signal+background model.","category":"page"},{"location":"tutorial/#Input-files","page":"Tutorial","title":"Input files","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"First of all, we have to populate partitions and events JSON input files. These dictionaries can be built manually by each user, but some reference files are already present for GERDA/LEGEND/MJD experiments at a private location (ask Toby Dixon for access to the repository). Some of these files were also copied in the inputs/ folder for already published material. The following files can be used for this tutorial:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"GERDA Phase I Events -> \"inputs/events_gerda_pI.json\"\nGERDA Phase I Partitions -> \"inputs/partitions_gerda_pI.json\"","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Notice that in \"inputs/partitions_gerda_pI.json\" we already specify in which range we want to fit data (here it is set common to all fit groups, i.e. [1930,2099] U [2109,2114] U [2124,2190] keV) and how we want to group different detectors. In particular, 4 fit groups are specified, with names ph1_golden, ph1_silver, phI_bege and phI_extra. For each group, we will have a separate background index (BI): mathcalB_phI-golden, mathcalB_phI-silver, mathcalB_phI-bege, mathcalB_phI-extra. In case you want one BI only for all phase I events, mathcalB_phI-all, then you have to modify the partitions input file to account for that.","category":"page"},{"location":"tutorial/#Fit-configuration","page":"Tutorial","title":"Fit configuration","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let's fit GERDA Phase I events with a simple signal+background=S+B model (\"bkg_only\": false) where the signal is modelled with a Gaussian function (default option).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The signal prior is taken as uniform (\"signal\": {\"prior\": \"uniform\", ...}) in 010^-24 yr^-1. Notice that for the signal the values are expressed in terms of 10^-27yr^-1 (that is why \"signal\": {\"upper_bound\": 1000, ...}). The BI prior is taken as uniform (\"bkg\": {\"prior\": \"uniform\", ...}) in 001 counts/keV/kg/yr.  Signal and background are always defined as positive, i.e. the lower bound is set to 0 by default.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We assume the 4 BIs are not correlated (\"bkg\": {\"correlated\": {\"mode\": \"none\", \"range\": \"none\"}}). If they are, change the entry into \"bkg\": {\"correlated\": {\"mode\": x, \"range\": [...,...]}} where x={\"lognormal\", \"normal\"}. These are the only hierarchical models present at the moment. More documentation on this topic can be found in \"A. Gelman, J.B. Carlin, H.S. Stern, D.B. Dunson, A. Vehtari and D.B. Rubin, “Bayesian Data Analysis (3rd ed.)”, Chapman & Hall (2013)\".","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"As regards the nuisance parameters, we can leave free the energy biases and energy widths by not fixing them to their best value (\"nuisance\": {\"energy_scale\": {\"fixed\": false, ...}, ...}), but constraining them via a Gaussian prior. An additional option for treating all energy biases or widths together via one common parameter, i.e. alpha_Delta or alpha_omega, can be enabled/disabled. For the moment, we leave this out and we treat nuisance parameters individually (\"nuisance\": {\"energy_scale\": {\"correlated\": false, ...}, ...}).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"As regards the efficiencies, we don't fix the values to their best value (\"nuisance\": {\"efficiency\": {\"fixed\": false, ...}, ...}), but we correlated them via a global parameter alpha_varepsilon (\"nuisance\": {\"efficiency\": {\"correlated\": true, ...}, ...}).","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"All these fit settings can therefore be grouped in the following config JSON file with name config_gerda_phI.json: ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"{\n    \"debug\":false,\n    \"events\":    [\"inputs/events_gerda_pI.json\"],\n    \"partitions\":[\"inputs/partitions_gerda_pI.json\"],\n    \"output_path\": \"output/fit_gerda_phI/\",\n    \"overwrite\": true,\n    \"light_output\": false,\n    \"bat_fit\": {\"nsteps\": 1e6, \"nchains\": 6},\n    \"plot\": {\"fit_and_data\": false, \"bandfit_and_data\": false, \"scheme\":\"green\", \"alpha\": 0.3},\n    \"bkg_only\": false,\n    \"signal\": {\"upper_bound\": 1000, \"prior\": \"uniform\"},\n    \"bkg\": {\"units\": \"ckky\", \"upper_bound\": 0.1, \"prior\": \"uniform\", \"correlated\": {\"mode\": \"none\", \"range\": \"none\"}},\n    \"nuisance\": { \n        \"energy_scale\" : {\n            \"correlated\": false,\n            \"fixed\": false\n        },\n        \"efficiency\" : {\n            \"correlated\": true,\n            \"fixed\": false\n        }\n    }\n}","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"You can now run this fit by running","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"$ julia main.jl -c config_gerda_phI.json","category":"page"},{"location":"tutorial/#GERDA-Phase-I-and-II:-SB-fit","page":"Tutorial","title":"GERDA Phase I and II: S+B fit","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Let's add data acquired by the GERDA experiment during its Phase II and let's fit them with a S+B model, leaving previous settings invariate.","category":"page"},{"location":"tutorial/#Input-files-2","page":"Tutorial","title":"Input files","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In the partitions file, we group detectors from Phase II in one fit group only (i.e. all_phase_II) such that they all share one BI, i.e. mathcalB_phII-all.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"GERDA Phase II events -> \"inputs/events_gerda_pII.json\"\nGERDA Phase II partitions -> \"inputs/partitions_gerda_pII.json\"","category":"page"},{"location":"tutorial/#Fit-configuration-2","page":"Tutorial","title":"Fit configuration","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Combined fits of different phases of one experiment (or, from a more general point of view, combined fits of different experiments) can be achieved by introducing new events and partitions files to the config JSON file.  Let's create the following config JSON file with name config_gerda_phIandphII.json: ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"{\n    ...,\n    \"events\":    [\"inputs/events_gerda_pI.json\", \"inputs/events_gerda_pII.json\"],\n    \"partitions\":[\"inputs/partitions_gerda_pI.json\", \"inputs/partitions_gerda_pII.json\"],\n    \"output_path\": \"output/fit_gerda_phIandphII/\",\n    ...\n}","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"All the rest can be left unchanged and you can now run the GERDA Phase I+II combined fit by running","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"$ julia main.jl -c config_gerda_phIandphII.json","category":"page"},{"location":"tutorial/#GERDA-Phase-I-and-II:-SB-fit,-non-flat-background","page":"Tutorial","title":"GERDA Phase I and II: S+B fit, non flat background","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The background is assumed flat by default, but how can we include a potential different shape? A linear or exponential background shape were implemented as well, and one of these shapes can be specified under the \"bkg\" key. Let's create the following config JSON file with name config_gerda_phIandphII_linearB.json: ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"{\n    ...,\n    \"events\":    [\"inputs/events_gerda_pI.json\", \"inputs/events_gerda_pII.json\"],\n    \"partitions\":[\"inputs/partitions_gerda_pI.json\", \"inputs/partitions_gerda_pII.json\"],\n    \"output_path\": \"output/fit_gerda_phIandphII_linearB/\",\n    \"bkg\": {\n        \"units\": \"ckky\", \n        \"upper_bound\": 0.1, \n        \"prior\": \"uniform\",\n        \"correlated\": {\"mode\": \"none\", \"range\": \"none\"},\n        \"shape\":{\n            \"name\":\"linear\",\n            \"pars\":{\"slope\":[-1,3]}\n        }\n    },\n    ...\n}","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"or config_gerda_phIandphII_expoB.json:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"{\n    ...,\n    \"events\":    [\"inputs/events_gerda_pI.json\", \"inputs/events_gerda_pII.json\"],\n    \"partitions\":[\"inputs/partitions_gerda_pI.json\", \"inputs/partitions_gerda_pII.json\"],\n    \"output_path\": \"output/fit_gerda_phIandphII_expoB/\",\n    \"bkg\": {\n        \"units\": \"ckky\", \n        \"upper_bound\": 0.1, \n        \"prior\": \"uniform\",\n        \"correlated\": {\"mode\": \"none\", \"range\": \"none\"},\n        \"shape\":{\n            \"name\":\"exponential\",\n            \"pars\":{\"slope\":[-10,10]}\n        }\n    },\n    ...\n}","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"You can now run this fit by running","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"$ julia main.jl -c config_gerda_phIandphII_linearB.json","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"if you want to shape the background with a linear function, or ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"$ julia main.jl -c config_gerda_phIandphII_expoB.json","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"if you want to shape the background with an exponential function.","category":"page"},{"location":"tutorial/#GERDA-Phase-I-and-II:-B-only-fit","page":"Tutorial","title":"GERDA Phase I and II: B only fit","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Sometimes it is required to fit under the mathcalS=0 (no signal) assumption. This might be helpful when performing sensitivity studies in the context of 0nubetabeta decay analyses. This can be achieved by setting \"bkg_only\": true in our config JSON file that we now call config_gerda_phIandphII_NoSignal.json:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"{\n    ...,\n    \"events\":    [\"inputs/events_gerda_pI.json\", \"inputs/events_gerda_pII.json\"],\n    \"partitions\":[\"inputs/partitions_gerda_pI.json\", \"inputs/partitions_gerda_pII.json\"],\n    \"output_path\": \"output/fit_gerda_phIandphII_NoSignal/\",\n    \"bkg_only\": true,\n    ...\n}","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"You can now run this fit by running","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"$ julia main.jl -c config_gerda_phIandphII_NoSignal.json","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Additional details on the type of available sensitivity studies (e.g. how to generate fake spectra and fit them) can be found in the \"Generating toys\" section.","category":"page"},{"location":"tutorial/#GERDA-MJD:-different-S-shapes","page":"Tutorial","title":"GERDA + MJD: different S shapes","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"As explained in \"I. J. Arnquist et al., Final Result of the Majorana Demonstrator’s Search for Neutrinoless Double- β Decay in Ge 76, PRL 130, 062501 (2023)\", MJD used a modified Gaussian signal peak shape. The code can take care of this difference once you specify the type of signal shape one wants to use in the partitions file. Use \"signal_name\": \"gaussian_plus_lowEtail\" for MJD, and \"signal_name\": \"gaussian\" (or don't enter any key - this is the default option) for GERDA partitions. Below we report an example of how the MJD partitions JSON file should look like for changing the Gaussian signal function into a modified one. Notice that for MJD we can also specify a different fit range (that now includes an additional window in 2209.1-2350.0 keV) compared to the one used by GERDA.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"    {\n        \"fit_groups\": {\n            \"mjd-DS0\": {\n                \"range\": [[1950.0, 2098.511], [2108.511, 2113.513], [2123.513, 2199.1], [2209.1, 2350.0]],\n                \"model\": \"uniform\",\n                \"bkg_name\": \"B_mjd-DS0\",\n                \"signal_name\": \"gaussian_plus_lowEtail\"\n            }\n        },\n        \"partitions\": {\n            \"mjd-DS0\": [...]\n        }\n    }","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"The aim of this tutorial consists in building proper config JSON files in order to run a neutrinoless double-beta decay analysis over GERDA and MAJORANA DEMONSTRATO (MJD) published data. Additional info on the meaning of input parameters can be found under the \"Configuration file\" section, and for input files under the \"Partitions and events\" section.","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"Table of contents:","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"Pages = [\"tutorial.md\"]\nDepth = 3","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/#GERDA-Phase-I:-SB-fit","page":"Tutorial","title":"GERDA Phase I: S+B fit","text":"","category":"section"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"Let's start by fitting data acquired by the GERDA experiment during its Phase I and let's fit them with a signal+background model.","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/#Input-files","page":"Tutorial","title":"Input files","text":"","category":"section"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"First of all, we have to populate partitions and events JSON input files. These dictionaries can be built manually by each user, but some reference files are already present for GERDA/LEGEND/MJD experiments at a private location (ask Toby Dixon for access to the repository). Some of these files were also copied in the inputs/ folder for already published material. The following files can be used for this tutorial:","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"GERDA Phase I Events -> \"inputs/events_gerda_pI.json\"\nGERDA Phase I Partitions -> \"inputs/partitions_gerda_pI.json\"","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"Notice that in \"inputs/partitions_gerda_pI.json\" we already specify in which range we want to fit data (here it is set common to all fit groups, i.e. [1930,2099] U [2109,2114] U [2124,2190] keV) and how we want to group different detectors. In particular, 4 fit groups are specified, with names ph1_golden, ph1_silver, phI_bege and phI_extra. For each group, we will have a separate background index (BI): mathcalB_phI-golden, mathcalB_phI-silver, mathcalB_phI-bege, mathcalB_phI-extra. In case you want one BI only for all phase I events, mathcalB_phI-all, then you have to modify the partitions input file to account for that.","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/#Fit-configuration","page":"Tutorial","title":"Fit configuration","text":"","category":"section"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"Let's fit GERDA Phase I events with a simple signal+background=S+B model (\"bkg_only\": false) where the signal is modelled with a Gaussian function (default option).","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"The signal prior is taken as uniform (\"signal\": {\"prior\": \"uniform\", ...}) in 010^-24 yr^-1. Notice that for the signal the values are expressed in terms of 10^-27yr^-1 (that is why \"signal\": {\"upper_bound\": 1000, ...}). The BI prior is taken as uniform (\"bkg\": {\"prior\": \"uniform\", ...}) in 001 counts/keV/kg/yr.  Signal and background are always defined as positive, i.e. the lower bound is set to 0 by default.","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"We assume the 4 BIs are not correlated (\"bkg\": {\"correlated\": {\"mode\": \"none\", \"range\": \"none\"}}). If they are, change the entry into \"bkg\": {\"correlated\": {\"mode\": x, \"range\": [...,...]}} where x={\"lognormal\", \"normal\"}. These are the only hierarchical models present at the moment. More documentation on this topic can be found in \"A. Gelman, J.B. Carlin, H.S. Stern, D.B. Dunson, A. Vehtari and D.B. Rubin, “Bayesian Data Analysis (3rd ed.)”, Chapman & Hall (2013)\".","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"As regards the nuisance parameters, we can leave free the energy biases and energy widths by not fixing them to their best value (\"nuisance\": {\"energy_scale\": {\"fixed\": false, ...}, ...}), but constraining them via a Gaussian prior. An additional option for treating all energy biases or widths together via one common parameter, i.e. alpha_Delta or alpha_omega, can be enabled/disabled. For the moment, we leave this out and we treat nuisance parameters individually (\"nuisance\": {\"energy_scale\": {\"correlated\": false, ...}, ...}).","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"As regards the efficiencies, we don't fix the values to their best value (\"nuisance\": {\"efficiency\": {\"fixed\": false, ...}, ...}), but we correlated them via a global parameter alpha_varepsilon (\"nuisance\": {\"efficiency\": {\"correlated\": true, ...}, ...}).","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"All these fit settings can therefore be grouped in the following config JSON file with name config_gerda_phI.json: ","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"{\n    \"debug\":false,\n    \"events\":    [\"inputs/events_gerda_pI.json\"],\n    \"partitions\":[\"inputs/partitions_gerda_pI.json\"],\n    \"output_path\": \"output/fit_gerda_phI/\",\n    \"overwrite\": true,\n    \"light_output\": false,\n    \"bat_fit\": {\"nsteps\": 1e6, \"nchains\": 6},\n    \"plot\": {\"fit_and_data\": false, \"bandfit_and_data\": false, \"scheme\":\"green\", \"alpha\": 0.3},\n    \"bkg_only\": false,\n    \"signal\": {\"upper_bound\": 1000, \"prior\": \"uniform\"},\n    \"bkg\": {\"units\": \"ckky\", \"upper_bound\": 0.1, \"prior\": \"uniform\", \"correlated\": {\"mode\": \"none\", \"range\": \"none\"}},\n    \"nuisance\": { \n        \"energy_scale\" : {\n            \"correlated\": false,\n            \"fixed\": false\n        },\n        \"efficiency\" : {\n            \"correlated\": true,\n            \"fixed\": false\n        }\n    }\n}","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"You can now run this fit by running","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"$ julia main.jl -c config_gerda_phI.json","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/#GERDA-Phase-I-and-II:-SB-fit","page":"Tutorial","title":"GERDA Phase I and II: S+B fit","text":"","category":"section"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"Let's add data acquired by the GERDA experiment during its Phase II and let's fit them with a S+B model, leaving previous settings invariate.","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/#Input-files-2","page":"Tutorial","title":"Input files","text":"","category":"section"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"In the partitions file, we group detectors from Phase II in one fit group only (i.e. all_phase_II) such that they all share one BI, i.e. mathcalB_phII-all.","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"GERDA Phase II events -> \"inputs/events_gerda_pII.json\"\nGERDA Phase II partitions -> \"inputs/partitions_gerda_pII.json\"","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/#Fit-configuration-2","page":"Tutorial","title":"Fit configuration","text":"","category":"section"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"Combined fits of different phases of one experiment (or, from a more general point of view, combined fits of different experiments) can be achieved by introducing new events and partitions files to the config JSON file.  Let's create the following config JSON file with name config_gerda_phIandphII.json: ","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"{\n    ...,\n    \"events\":    [\"inputs/events_gerda_pI.json\", \"inputs/events_gerda_pII.json\"],\n    \"partitions\":[\"inputs/partitions_gerda_pI.json\", \"inputs/partitions_gerda_pII.json\"],\n    \"output_path\": \"output/fit_gerda_phIandphII/\",\n    ...\n}","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"All the rest can be left unchanged and you can now run the GERDA Phase I+II combined fit by running","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"$ julia main.jl -c config_gerda_phIandphII.json","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/#GERDA-Phase-I-and-II:-SB-fit,-non-flat-background","page":"Tutorial","title":"GERDA Phase I and II: S+B fit, non flat background","text":"","category":"section"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"The background is assumed flat by default, but how can we include a potential different shape? A linear or exponential background shape were implemented as well, and one of these shapes can be specified under the \"bkg\" key. Let's create the following config JSON file with name config_gerda_phIandphII_linearB.json: ","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"{\n    ...,\n    \"events\":    [\"inputs/events_gerda_pI.json\", \"inputs/events_gerda_pII.json\"],\n    \"partitions\":[\"inputs/partitions_gerda_pI.json\", \"inputs/partitions_gerda_pII.json\"],\n    \"output_path\": \"output/fit_gerda_phIandphII_linearB/\",\n    \"bkg\": {\n        \"units\": \"ckky\", \n        \"upper_bound\": 0.1, \n        \"prior\": \"uniform\",\n        \"correlated\": {\"mode\": \"none\", \"range\": \"none\"},\n        \"shape\":{\n            \"name\":\"linear\",\n            \"pars\":{\"slope\":[-1,3]}\n        }\n    },\n    ...\n}","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"or config_gerda_phIandphII_expoB.json:","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"{\n    ...,\n    \"events\":    [\"inputs/events_gerda_pI.json\", \"inputs/events_gerda_pII.json\"],\n    \"partitions\":[\"inputs/partitions_gerda_pI.json\", \"inputs/partitions_gerda_pII.json\"],\n    \"output_path\": \"output/fit_gerda_phIandphII_expoB/\",\n    \"bkg\": {\n        \"units\": \"ckky\", \n        \"upper_bound\": 0.1, \n        \"prior\": \"uniform\",\n        \"correlated\": {\"mode\": \"none\", \"range\": \"none\"},\n        \"shape\":{\n            \"name\":\"exponential\",\n            \"pars\":{\"slope\":[-10,10]}\n        }\n    },\n    ...\n}","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"You can now run this fit by running","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"$ julia main.jl -c config_gerda_phIandphII_linearB.json","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"if you want to shape the background with a linear function, or ","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"$ julia main.jl -c config_gerda_phIandphII_expoB.json","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"if you want to shape the background with an exponential function.","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/#GERDA-Phase-I-and-II:-B-only-fit","page":"Tutorial","title":"GERDA Phase I and II: B only fit","text":"","category":"section"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"Sometimes it is required to fit under the mathcalS=0 (no signal) assumption. This might be helpful when performing sensitivity studies in the context of 0nubetabeta decay analyses. This can be achieved by setting \"bkg_only\": true in our config JSON file that we now call config_gerda_phIandphII_NoSignal.json:","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"{\n    ...,\n    \"events\":    [\"inputs/events_gerda_pI.json\", \"inputs/events_gerda_pII.json\"],\n    \"partitions\":[\"inputs/partitions_gerda_pI.json\", \"inputs/partitions_gerda_pII.json\"],\n    \"output_path\": \"output/fit_gerda_phIandphII_NoSignal/\",\n    \"bkg_only\": true,\n    ...\n}","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"You can now run this fit by running","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"$ julia main.jl -c config_gerda_phIandphII_NoSignal.json","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"Additional details on the type of available sensitivity studies (e.g. how to generate fake spectra and fit them) can be found in the \"Generating toys\" section.","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/#GERDA-MJD:-different-S-shapes","page":"Tutorial","title":"GERDA + MJD: different S shapes","text":"","category":"section"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"As explained in \"I. J. Arnquist et al., Final Result of the Majorana Demonstrator’s Search for Neutrinoless Double- β Decay in Ge 76, PRL 130, 062501 (2023)\", MJD used a modified Gaussian signal peak shape. The code can take care of this difference once you specify the type of signal shape one wants to use in the partitions file. Use \"signal_name\": \"gaussian_plus_lowEtail\" for MJD, and \"signal_name\": \"gaussian\" (or don't enter any key - this is the default option) for GERDA partitions. Below we report an example of how the MJD partitions JSON file should look like for changing the Gaussian signal function into a modified one. Notice that for MJD we can also specify a different fit range (that now includes an additional window in 2209.1-2350.0 keV) compared to the one used by GERDA.","category":"page"},{"location":".ipynb_checkpoints/tutorial-checkpoint/","page":"Tutorial","title":"Tutorial","text":"    {\n        \"fit_groups\": {\n            \"mjd-DS0\": {\n                \"range\": [[1950.0, 2098.511], [2108.511, 2113.513], [2123.513, 2199.1], [2209.1, 2350.0]],\n                \"model\": \"uniform\",\n                \"bkg_name\": \"B_mjd-DS0\",\n                \"signal_name\": \"gaussian_plus_lowEtail\"\n            }\n        },\n        \"partitions\": {\n            \"mjd-DS0\": [...]\n        }\n    }","category":"page"}]
}
